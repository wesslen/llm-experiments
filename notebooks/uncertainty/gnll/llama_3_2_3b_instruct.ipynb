{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOwkp3aMY+jfRSZ67NIO5cR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/uncertainty/gnll/llama_3_2_3b_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWSLErS567GJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@dataclass\n",
        "class UncertaintyEstimate:\n",
        "    \"\"\"Container for various uncertainty metrics\"\"\"\n",
        "    g_nll: float  # Greedy Negative Log-Likelihood\n",
        "    predictive_entropy: Optional[float] = None\n",
        "    semantic_entropy: Optional[float] = None\n",
        "    length_normalized_pe: Optional[float] = None\n",
        "    length_normalized_se: Optional[float] = None\n",
        "    discrete_se: Optional[float] = None\n",
        "\n",
        "class LLMUncertaintyAnalyzer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        nli_model_name: str = \"microsoft/deberta-v3-large\",\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        use_fp16: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize uncertainty analyzer with models for generation and semantic similarity.\n",
        "\n",
        "        Args:\n",
        "            model_name: HuggingFace model identifier for the main LLM\n",
        "            nli_model_name: Model for semantic similarity scoring\n",
        "            device: Computing device (\"cuda\" or \"cpu\")\n",
        "            use_fp16: Whether to use FP16 precision\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.dtype = torch.float16 if use_fp16 and device == \"cuda\" else torch.float32\n",
        "\n",
        "        print(\"Initializing tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        print(\"Initializing main model...\")\n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=self.dtype,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None\n",
        "            ).to(device)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading main model: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Set up padding and attention mask\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
        "        print(\"Initializing NLI model and tokenizer...\")\n",
        "        try:\n",
        "            self.nli_model = AutoModel.from_pretrained(nli_model_name).to(device)\n",
        "            self.nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading NLI model: {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"Initialization complete.\")\n",
        "\n",
        "    def calculate_g_nll(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 100,\n",
        "        num_beams: int = 1  # 1 for greedy decoding\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Calculate G-NLL (Greedy Negative Log-Likelihood) uncertainty measure.\n",
        "        Uses greedy or beam search decoding to approximate the most likely sequence.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text prompt\n",
        "            max_length: Maximum generation length\n",
        "            num_beams: Number of beams for beam search (1 for greedy)\n",
        "\n",
        "        Returns:\n",
        "            G-NLL uncertainty score (higher means more uncertain)\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            # Get log probabilities for the generated sequence\n",
        "            scores = torch.stack(outputs.scores)\n",
        "            log_probs = F.log_softmax(scores, dim=-1)\n",
        "\n",
        "            # Get the maximum log probability for each position\n",
        "            max_log_probs = torch.max(log_probs, dim=-1).values\n",
        "\n",
        "            # Calculate G-NLL as negative sum of max log probs\n",
        "            g_nll = -torch.sum(max_log_probs).item()\n",
        "\n",
        "        return g_nll\n",
        "\n",
        "    def calculate_semantic_similarity(\n",
        "        self,\n",
        "        text1: str,\n",
        "        text2: str\n",
        "    ) -> float:\n",
        "        \"\"\"Calculate semantic similarity between two texts using the NLI model.\"\"\"\n",
        "        # Encode each text separately\n",
        "        encoding1 = self.nli_tokenizer(\n",
        "            text1,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "\n",
        "        encoding2 = self.nli_tokenizer(\n",
        "            text2,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get embeddings for each text\n",
        "            outputs1 = self.nli_model(**encoding1)\n",
        "            outputs2 = self.nli_model(**encoding2)\n",
        "\n",
        "            # Mean pool the last hidden states\n",
        "            embedding1 = outputs1.last_hidden_state.mean(dim=1)\n",
        "            embedding2 = outputs2.last_hidden_state.mean(dim=1)\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = F.cosine_similarity(embedding1, embedding2).item()\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def calculate_predictive_entropy(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        num_samples: int = 10,\n",
        "        max_length: int = 100,\n",
        "        temperature: float = 0.8\n",
        "    ) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Calculate Predictive Entropy (PE) and Length-normalized PE.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (PE, Length-normalized PE)\n",
        "        \"\"\"\n",
        "        log_probs_list = []\n",
        "        lengths = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device),\n",
        "                    max_length=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=temperature,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "\n",
        "                scores = torch.stack(outputs.scores)\n",
        "                log_probs = F.log_softmax(scores, dim=-1)\n",
        "                selected_log_probs = torch.max(log_probs, dim=-1).values\n",
        "                log_probs_list.append(selected_log_probs.sum().item())\n",
        "                lengths.append(len(selected_log_probs))\n",
        "\n",
        "        # Calculate standard PE\n",
        "        pe = -np.mean(log_probs_list)\n",
        "\n",
        "        # Calculate length-normalized PE\n",
        "        normalized_log_probs = [lp / length for lp, length in zip(log_probs_list, lengths)]\n",
        "        ln_pe = -np.mean(normalized_log_probs)\n",
        "\n",
        "        return pe, ln_pe\n",
        "\n",
        "    def calculate_semantic_entropy(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        num_samples: int = 10,\n",
        "        max_length: int = 100,\n",
        "        temperature: float = 0.8,\n",
        "        similarity_threshold: float = 0.8\n",
        "    ) -> Tuple[float, float, float]:\n",
        "        \"\"\"\n",
        "        Calculate Semantic Entropy (SE), Length-normalized SE, and Discrete SE.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (SE, Length-normalized SE, Discrete SE)\n",
        "        \"\"\"\n",
        "        # Generate samples\n",
        "        samples = []\n",
        "        for _ in range(num_samples):\n",
        "            with torch.no_grad():\n",
        "                output_ids = self.model.generate(\n",
        "                    self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device),\n",
        "                    max_length=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                samples.append(self.tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n",
        "        # Calculate semantic similarity matrix\n",
        "        sim_matrix = np.zeros((num_samples, num_samples))\n",
        "        for i in range(num_samples):\n",
        "            for j in range(i + 1, num_samples):\n",
        "                sim = self.calculate_semantic_similarity(samples[i], samples[j])\n",
        "                sim_matrix[i, j] = sim_matrix[j, i] = sim\n",
        "\n",
        "        # Cluster samples based on similarity\n",
        "        clusters = []\n",
        "        used = set()\n",
        "        for i in range(num_samples):\n",
        "            if i not in used:\n",
        "                cluster = {i}\n",
        "                used.add(i)\n",
        "                for j in range(num_samples):\n",
        "                    if j not in used and sim_matrix[i, j] >= similarity_threshold:\n",
        "                        cluster.add(j)\n",
        "                        used.add(j)\n",
        "                clusters.append(cluster)\n",
        "\n",
        "        # Calculate different forms of semantic entropy\n",
        "        cluster_probs = [len(c) / num_samples for c in clusters]\n",
        "        se = entropy(cluster_probs)\n",
        "        ln_se = se / len(clusters)  # Length-normalized SE\n",
        "        d_se = 1 - (1 / len(clusters))  # Discrete SE\n",
        "\n",
        "        return se, ln_se, d_se\n",
        "\n",
        "    def estimate_uncertainty(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 100,\n",
        "        num_samples: int = 10,\n",
        "        temperature: float = 0.8,\n",
        "        calculate_all: bool = True\n",
        "    ) -> UncertaintyEstimate:\n",
        "        \"\"\"\n",
        "        Calculate all uncertainty measures for a given prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text prompt\n",
        "            max_length: Maximum generation length\n",
        "            num_samples: Number of samples for entropy-based measures\n",
        "            temperature: Sampling temperature\n",
        "            calculate_all: Whether to calculate all measures or just G-NLL\n",
        "\n",
        "        Returns:\n",
        "            UncertaintyEstimate object containing all calculated metrics\n",
        "        \"\"\"\n",
        "        # Always calculate G-NLL as our primary measure\n",
        "        g_nll = self.calculate_g_nll(prompt, max_length)\n",
        "\n",
        "        if not calculate_all:\n",
        "            return UncertaintyEstimate(g_nll=g_nll)\n",
        "\n",
        "        # Calculate additional measures if requested\n",
        "        pe, ln_pe = self.calculate_predictive_entropy(\n",
        "            prompt, num_samples, max_length, temperature\n",
        "        )\n",
        "\n",
        "        se, ln_se, d_se = self.calculate_semantic_entropy(\n",
        "            prompt, num_samples, max_length, temperature\n",
        "        )\n",
        "\n",
        "        return UncertaintyEstimate(\n",
        "            g_nll=g_nll,\n",
        "            predictive_entropy=pe,\n",
        "            length_normalized_pe=ln_pe,\n",
        "            semantic_entropy=se,\n",
        "            length_normalized_se=ln_se,\n",
        "            discrete_se=d_se\n",
        "        )\n",
        "\n",
        "def evaluate_uncertainty(\n",
        "    analyzer: LLMUncertaintyAnalyzer,\n",
        "    prompts: List[str],\n",
        "    ground_truth: List[str],\n",
        "    f1_threshold: float = 0.5\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate uncertainty estimation performance using AUROC.\n",
        "\n",
        "    Args:\n",
        "        analyzer: Initialized LLMUncertaintyAnalyzer\n",
        "        prompts: List of input prompts\n",
        "        ground_truth: List of ground truth answers\n",
        "        f1_threshold: Threshold for F1 score to consider an answer correct\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of AUROC scores for each uncertainty measure\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "    results = []\n",
        "    correctness = []\n",
        "\n",
        "    for prompt, truth in tqdm(zip(prompts, ground_truth), total=len(prompts)):\n",
        "        # Get uncertainty estimates\n",
        "        uncertainty = analyzer.estimate_uncertainty(prompt, calculate_all=True)\n",
        "        results.append(uncertainty)\n",
        "\n",
        "        # Generate answer for correctness evaluation\n",
        "        output_ids = analyzer.model.generate(\n",
        "            analyzer.tokenizer.encode(prompt, return_tensors=\"pt\").to(analyzer.device),\n",
        "            max_length=100,\n",
        "            num_beams=1  # Greedy decoding\n",
        "        )\n",
        "        answer = analyzer.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate F1-like score using BLEU (simplified for example)\n",
        "        score = sentence_bleu([truth.split()], answer.split())\n",
        "        correctness.append(score >= f1_threshold)\n",
        "\n",
        "    # Calculate AUROC for each measure\n",
        "    auroc_scores = {}\n",
        "    for field in UncertaintyEstimate.__dataclass_fields__:\n",
        "        if field == \"g_nll\":  # Always available\n",
        "            values = [getattr(r, field) for r in results]\n",
        "            auroc_scores[field] = roc_auc_score(correctness, values)\n",
        "        else:  # Optional measures\n",
        "            values = [getattr(r, field) for r in results if getattr(r, field) is not None]\n",
        "            if values:\n",
        "                auroc_scores[field] = roc_auc_score(\n",
        "                    correctness[:len(values)], values\n",
        "                )\n",
        "\n",
        "    return auroc_scores\n",
        "\n",
        "def test_uncertainty_estimation(\n",
        "    model_name: str = \"gpt2\",  # Using smaller model by default\n",
        "    nli_model_name: str = \"microsoft/deberta-v3-small\",\n",
        "    test_prompt: str = \"What is the capital of France?\",\n",
        "    use_fp16: bool = True,\n",
        "    max_length: int = 50,\n",
        "    num_samples: int = 5\n",
        "):\n",
        "    \"\"\"\n",
        "    Test function for uncertainty estimation.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the main LLM model\n",
        "        nli_model_name: Name of the NLI model for semantic similarity\n",
        "        test_prompt: Prompt to test\n",
        "        use_fp16: Whether to use half precision\n",
        "        max_length: Maximum generation length\n",
        "        num_samples: Number of samples for entropy calculation\n",
        "    \"\"\"\n",
        "    print(f\"Initializing with model: {model_name}\")\n",
        "    print(f\"Using NLI model: {nli_model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize analyzer\n",
        "        analyzer = LLMUncertaintyAnalyzer(\n",
        "            model_name=model_name,\n",
        "            nli_model_name=nli_model_name,\n",
        "            use_fp16=use_fp16\n",
        "        )\n",
        "\n",
        "        print(\"\\nTesting uncertainty estimation...\")\n",
        "        print(f\"Prompt: {test_prompt}\")\n",
        "\n",
        "        # Get uncertainty estimates\n",
        "        uncertainty = analyzer.estimate_uncertainty(\n",
        "            prompt=test_prompt,\n",
        "            max_length=max_length,\n",
        "            num_samples=num_samples,\n",
        "            calculate_all=True\n",
        "        )\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nResults:\")\n",
        "        print(f\"G-NLL: {uncertainty.g_nll:.4f}\")\n",
        "        if uncertainty.predictive_entropy is not None:\n",
        "            print(f\"Predictive Entropy: {uncertainty.predictive_entropy:.4f}\")\n",
        "        if uncertainty.semantic_entropy is not None:\n",
        "            print(f\"Semantic Entropy: {uncertainty.semantic_entropy:.4f}\")\n",
        "        if uncertainty.length_normalized_pe is not None:\n",
        "            print(f\"Length-normalized PE: {uncertainty.length_normalized_pe:.4f}\")\n",
        "        if uncertainty.length_normalized_se is not None:\n",
        "            print(f\"Length-normalized SE: {uncertainty.length_normalized_se:.4f}\")\n",
        "        if uncertainty.discrete_se is not None:\n",
        "            print(f\"Discrete SE: {uncertainty.discrete_se:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during testing: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Categories:\n",
        "\n",
        "1.   Factual: Simple, verifiable facts\n",
        "2.   Opinion: Subjective questions\n",
        "3.   Creative: Open-ended creative tasks\n",
        "4.   Ambiguous: Philosophical/abstract questions\n",
        "5.   Technical: Complex technical explanations\n",
        "6.   Arithmetic: Mathematical calculations\n",
        "\n",
        "## Expected Patterns:\n",
        "\n",
        "- G-NLL should be lowest for factual and arithmetic questions\n",
        "- Semantic Entropy should be highest for creative and opinion prompts\n",
        "- Predictive Entropy might be high for ambiguous questions\n",
        "- Length-normalized measures should help compare across different response lengths\n",
        "\n",
        "## Expected insights:\n",
        "\n",
        "### Factual questions should show:\n",
        "\n",
        "- Lower G-NLL (more confident)\n",
        "- Lower Semantic Entropy (consistent meanings)\n",
        "- Lower Predictive Entropy (consistent outputs)\n",
        "\n",
        "\n",
        "### Creative/Opinion questions should show:\n",
        "\n",
        "- Higher G-NLL (less confident)\n",
        "- Higher Semantic Entropy (varied meanings)\n",
        "- Higher Predictive Entropy (varied outputs)\n",
        "\n",
        "\n",
        "### Technical questions might show:\n",
        "\n",
        "- Moderate G-NLL\n",
        "- Low Semantic Entropy\n",
        "- High Predictive Entropy (many ways to explain)"
      ],
      "metadata": {
        "id": "HJewR0emAEIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Hugging Face token from the Colab secret\n",
        "hf_token = userdata.get('huggingface')\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "DOBquJZJC-w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "class UncertaintyExperiment:\n",
        "    def __init__(self, analyzer):\n",
        "        self.analyzer = analyzer\n",
        "\n",
        "    def run_experiment(self, prompts: Dict[str, List[str]], max_length: int = 50):\n",
        "        \"\"\"\n",
        "        Run uncertainty analysis on different types of prompts.\n",
        "\n",
        "        Args:\n",
        "            prompts: Dictionary mapping prompt categories to lists of prompts\n",
        "            max_length: Maximum generation length\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for category, prompt_list in prompts.items():\n",
        "            print(f\"\\nProcessing {category} prompts...\")\n",
        "            for prompt in tqdm(prompt_list):\n",
        "                uncertainty = self.analyzer.estimate_uncertainty(\n",
        "                    prompt=prompt,\n",
        "                    max_length=max_length,\n",
        "                    calculate_all=True\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    'category': category,\n",
        "                    'prompt': prompt,\n",
        "                    'g_nll': uncertainty.g_nll,\n",
        "                    'predictive_entropy': uncertainty.predictive_entropy,\n",
        "                    'semantic_entropy': uncertainty.semantic_entropy,\n",
        "                    'length_normalized_pe': uncertainty.length_normalized_pe,\n",
        "                    'length_normalized_se': uncertainty.length_normalized_se,\n",
        "                    'discrete_se': uncertainty.discrete_se\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def visualize_results(self, df: pd.DataFrame, save_path: str = None):\n",
        "        \"\"\"Create visualizations for uncertainty measures across categories.\"\"\"\n",
        "        # Prepare data for plotting\n",
        "        measures = ['g_nll', 'predictive_entropy', 'semantic_entropy',\n",
        "                   'length_normalized_pe', 'length_normalized_se', 'discrete_se']\n",
        "\n",
        "        # Create subplots for each measure\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        fig.suptitle('Uncertainty Measures Across Different Prompt Types', fontsize=16)\n",
        "\n",
        "        for idx, measure in enumerate(measures):\n",
        "            ax = axes[idx // 3, idx % 3]\n",
        "            sns.boxplot(data=df, x='category', y=measure, ax=ax)\n",
        "            ax.set_title(measure)\n",
        "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def analyze_correlations(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Analyze correlations between different uncertainty measures and provide detailed statistics.\n",
        "        \"\"\"\n",
        "        measures = ['g_nll', 'predictive_entropy', 'semantic_entropy',\n",
        "                   'length_normalized_pe', 'length_normalized_se', 'discrete_se']\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = df[measures].corr()\n",
        "\n",
        "        # Calculate descriptive statistics for each measure by category\n",
        "        stats_by_category = {}\n",
        "        for category in df['category'].unique():\n",
        "            category_data = df[df['category'] == category][measures]\n",
        "            stats_by_category[category] = {\n",
        "                'mean': category_data.mean(),\n",
        "                'std': category_data.std(),\n",
        "                'min': category_data.min(),\n",
        "                'max': category_data.max(),\n",
        "                'median': category_data.median()\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'correlation_matrix': corr_matrix,\n",
        "            'statistics_by_category': stats_by_category\n",
        "        }\n",
        "\n",
        "def run_comprehensive_experiment():\n",
        "    \"\"\"Run a comprehensive experiment with different types of prompts.\"\"\"\n",
        "    # Define test prompts by category\n",
        "    test_prompts = {\n",
        "        'factual': [\n",
        "            \"What is the capital of France?\",\n",
        "            \"What year did World War II end?\",\n",
        "            \"What is the chemical symbol for gold?\",\n",
        "            \"Who wrote Romeo and Juliet?\",\n",
        "            \"What is the speed of light in meters per second?\"\n",
        "        ],\n",
        "        'opinion': [\n",
        "            \"What is the best way to learn a new language?\",\n",
        "            \"Why is democracy important?\",\n",
        "            \"What makes a good leader?\",\n",
        "            \"Is artificial intelligence beneficial for society?\",\n",
        "            \"What is the most important invention in history?\"\n",
        "        ],\n",
        "        'creative': [\n",
        "            \"Write a short story about a magical forest\",\n",
        "            \"Describe a futuristic city\",\n",
        "            \"Create a new superhero character\",\n",
        "            \"Write a poem about sunrise\",\n",
        "            \"Invent a new sport and describe its rules\"\n",
        "        ],\n",
        "        'ambiguous': [\n",
        "            \"Why do we dream?\",\n",
        "            \"What is consciousness?\",\n",
        "            \"How does time work?\",\n",
        "            \"What is the meaning of life?\",\n",
        "            \"What happens after death?\"\n",
        "        ],\n",
        "        'technical': [\n",
        "            \"Explain how a quantum computer works\",\n",
        "            \"Describe the process of photosynthesis\",\n",
        "            \"How does blockchain technology function?\",\n",
        "            \"Explain the theory of relativity\",\n",
        "            \"How does the human immune system work?\"\n",
        "        ],\n",
        "        'arithmetic': [\n",
        "            \"What is 2345 * 789?\",\n",
        "            \"Calculate the square root of 169\",\n",
        "            \"What is 15% of 430?\",\n",
        "            \"Solve for x: 3x + 7 = 22\",\n",
        "            \"Convert 157 kilometers to miles\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Initialize analyzer and experiment\n",
        "    # from llm_uncertainty import LLMUncertaintyAnalyzer\n",
        "    analyzer = LLMUncertaintyAnalyzer(\n",
        "        model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "        nli_model_name=\"microsoft/deberta-v3-small\"\n",
        "    )\n",
        "    experiment = UncertaintyExperiment(analyzer)\n",
        "\n",
        "    # Run experiment\n",
        "    print(\"Running uncertainty experiment...\")\n",
        "    results_df = experiment.run_experiment(test_prompts)\n",
        "\n",
        "    # Analyze results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    experiment.visualize_results(results_df, \"uncertainty_results.png\")\n",
        "\n",
        "    print(\"\\nAnalyzing correlations between measures...\")\n",
        "    correlations = experiment.analyze_correlations(results_df)\n",
        "\n",
        "    # Print summary statistics for numeric columns only\n",
        "    print(\"\\nSummary Statistics by Category:\")\n",
        "    numeric_columns = ['g_nll', 'predictive_entropy', 'semantic_entropy',\n",
        "                      'length_normalized_pe', 'length_normalized_se', 'discrete_se']\n",
        "    summary = results_df.groupby('category')[numeric_columns].mean()\n",
        "    print(summary)\n",
        "\n",
        "    # Calculate standard deviations\n",
        "    std_dev = results_df.groupby('category')[numeric_columns].std()\n",
        "    print(\"\\nStandard Deviations by Category:\")\n",
        "    print(std_dev)\n",
        "\n",
        "    return results_df, correlations\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_df, correlations = run_comprehensive_experiment()\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    print(\"=================\")\n",
        "\n",
        "    # Calculate and display statistics for each measure\n",
        "    measures = ['g_nll', 'predictive_entropy', 'semantic_entropy',\n",
        "               'length_normalized_pe', 'length_normalized_se', 'discrete_se']\n",
        "\n",
        "    for measure in measures:\n",
        "        print(f\"\\n{measure} Analysis:\")\n",
        "        print(\"-\" * (len(measure) + 9))\n",
        "\n",
        "        # Get statistics by category\n",
        "        stats = results_df.groupby('category')[measure].agg(['mean', 'std', 'min', 'max'])\n",
        "\n",
        "        # Find most and least uncertain categories\n",
        "        most_uncertain = stats['mean'].idxmax()\n",
        "        least_uncertain = stats['mean'].idxmin()\n",
        "\n",
        "        print(f\"Most uncertain category: {most_uncertain}\")\n",
        "        print(f\"  Mean: {stats.loc[most_uncertain, 'mean']:.4f}\")\n",
        "        print(f\"  Std Dev: {stats.loc[most_uncertain, 'std']:.4f}\")\n",
        "\n",
        "        print(f\"\\nLeast uncertain category: {least_uncertain}\")\n",
        "        print(f\"  Mean: {stats.loc[least_uncertain, 'mean']:.4f}\")\n",
        "        print(f\"  Std Dev: {stats.loc[least_uncertain, 'std']:.4f}\")\n",
        "\n",
        "        # Calculate relative uncertainty ratios\n",
        "        max_uncertainty = stats.loc[most_uncertain, 'mean']\n",
        "        min_uncertainty = stats.loc[least_uncertain, 'mean']\n",
        "        ratio = max_uncertainty / min_uncertainty if min_uncertainty != 0 else float('inf')\n",
        "        print(f\"\\nUncertainty ratio (most/least): {ratio:.2f}\")\n",
        "\n",
        "    # Print correlation insights\n",
        "    print(\"\\nCorrelation Analysis:\")\n",
        "    print(\"===================\")\n",
        "\n",
        "    high_correlations = []\n",
        "    for i in range(len(measures)):\n",
        "        for j in range(i+1, len(measures)):\n",
        "            corr = correlations['correlation_matrix'].iloc[i,j]\n",
        "            if abs(corr) > 0.5:  # Threshold for strong correlation\n",
        "                high_correlations.append((measures[i], measures[j], corr))\n",
        "\n",
        "    if high_correlations:\n",
        "        print(\"\\nStrong correlations found between:\")\n",
        "        for measure1, measure2, corr in sorted(high_correlations, key=lambda x: abs(x[2]), reverse=True):\n",
        "            print(f\"{measure1} and {measure2}: {corr:.3f}\")"
      ],
      "metadata": {
        "id": "u1aRKClv7B-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example result:\n",
        "\n",
        "```\n",
        "\n",
        "Analyzing correlations between measures...\n",
        "\n",
        "Summary Statistics by Category:\n",
        "               g_nll  predictive_entropy  semantic_entropy  \\\n",
        "category                                                     \n",
        "ambiguous   7.460395           10.947515          0.000000   \n",
        "arithmetic  5.238763            7.340768          0.127806   \n",
        "creative    8.621908           11.288671          0.000000   \n",
        "factual     5.116576            9.395902          0.000000   \n",
        "opinion     5.921459            9.509346          0.000000   \n",
        "technical   3.741592            6.405319          0.065017   \n",
        "\n",
        "            length_normalized_pe  length_normalized_se  discrete_se  \n",
        "category                                                             \n",
        "ambiguous               0.250374              0.000000     0.000000  \n",
        "arithmetic              0.183778              0.042602     0.133333  \n",
        "creative                0.262687              0.000000     0.000000  \n",
        "factual                 0.235652              0.000000     0.000000  \n",
        "opinion                 0.227618              0.000000     0.000000  \n",
        "technical               0.151334              0.032508     0.100000  \n",
        "\n",
        "Standard Deviations by Category:\n",
        "               g_nll  predictive_entropy  semantic_entropy  \\\n",
        "category                                                     \n",
        "ambiguous   3.438067            2.197581          0.000000   \n",
        "arithmetic  3.164331            3.265902          0.285784   \n",
        "creative    1.990587            2.570404          0.000000   \n",
        "factual     3.054828            2.344336          0.000000   \n",
        "opinion     2.475576            2.777571          0.000000   \n",
        "technical   2.069559            2.807816          0.145382   \n",
        "\n",
        "            length_normalized_pe  length_normalized_se  discrete_se  \n",
        "category                                                             \n",
        "ambiguous               0.051525              0.000000     0.000000  \n",
        "arithmetic              0.081125              0.095261     0.298142  \n",
        "creative                0.055048              0.000000     0.000000  \n",
        "factual                 0.053355              0.000000     0.000000  \n",
        "opinion                 0.060014              0.000000     0.000000  \n",
        "technical               0.064493              0.072691     0.223607  \n",
        "\n",
        "Detailed Analysis:\n",
        "=================\n",
        "\n",
        "g_nll Analysis:\n",
        "--------------\n",
        "Most uncertain category: creative\n",
        "  Mean: 8.6219\n",
        "  Std Dev: 1.9906\n",
        "\n",
        "Least uncertain category: technical\n",
        "  Mean: 3.7416\n",
        "  Std Dev: 2.0696\n",
        "\n",
        "Uncertainty ratio (most/least): 2.30\n",
        "\n",
        "predictive_entropy Analysis:\n",
        "---------------------------\n",
        "Most uncertain category: creative\n",
        "  Mean: 11.2887\n",
        "  Std Dev: 2.5704\n",
        "\n",
        "Least uncertain category: technical\n",
        "  Mean: 6.4053\n",
        "  Std Dev: 2.8078\n",
        "\n",
        "Uncertainty ratio (most/least): 1.76\n",
        "\n",
        "semantic_entropy Analysis:\n",
        "-------------------------\n",
        "Most uncertain category: arithmetic\n",
        "  Mean: 0.1278\n",
        "  Std Dev: 0.2858\n",
        "\n",
        "Least uncertain category: ambiguous\n",
        "  Mean: 0.0000\n",
        "  Std Dev: 0.0000\n",
        "\n",
        "Uncertainty ratio (most/least): inf\n",
        "\n",
        "length_normalized_pe Analysis:\n",
        "-----------------------------\n",
        "Most uncertain category: creative\n",
        "  Mean: 0.2627\n",
        "  Std Dev: 0.0550\n",
        "\n",
        "Least uncertain category: technical\n",
        "  Mean: 0.1513\n",
        "  Std Dev: 0.0645\n",
        "\n",
        "Uncertainty ratio (most/least): 1.74\n",
        "\n",
        "length_normalized_se Analysis:\n",
        "-----------------------------\n",
        "Most uncertain category: arithmetic\n",
        "  Mean: 0.0426\n",
        "  Std Dev: 0.0953\n",
        "\n",
        "Least uncertain category: ambiguous\n",
        "  Mean: 0.0000\n",
        "  Std Dev: 0.0000\n",
        "\n",
        "Uncertainty ratio (most/least): inf\n",
        "\n",
        "discrete_se Analysis:\n",
        "--------------------\n",
        "Most uncertain category: arithmetic\n",
        "  Mean: 0.1333\n",
        "  Std Dev: 0.2981\n",
        "\n",
        "Least uncertain category: ambiguous\n",
        "  Mean: 0.0000\n",
        "  Std Dev: 0.0000\n",
        "\n",
        "Uncertainty ratio (most/least): inf\n",
        "\n",
        "Correlation Analysis:\n",
        "===================\n",
        "\n",
        "Strong correlations found between:\n",
        "length_normalized_se and discrete_se: 1.000\n",
        "predictive_entropy and length_normalized_pe: 0.986\n",
        "semantic_entropy and discrete_se: 0.984\n",
        "semantic_entropy and length_normalized_se: 0.983\n",
        "g_nll and predictive_entropy: 0.722\n",
        "g_nll and length_normalized_pe: 0.705\n",
        "```"
      ],
      "metadata": {
        "id": "h2cL4hPHb_VS"
      }
    }
  ]
}