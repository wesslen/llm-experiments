{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZg1yaAvhop7TzsYOvTPV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/retrieval/synthetic_ir_hitrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJM_lk08LrzA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv pip install --system llama-index llama-index-embeddings-huggingface llama-index-llms-openai-like # llama-index-embeddings-instructor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('API_KEY')\n",
        "BASE_URL = userdata.get('BASE_URL')"
      ],
      "metadata": {
        "id": "6ZtqjwSULyLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take WF Code of Conduct\n",
        "!wget -O text.txt https://r.jina.ai/https://www.wellsfargo.com/assets/pdf/about/corporate/code-of-conduct.pdf"
      ],
      "metadata": {
        "id": "HuIGkYiLRJnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai_like import OpenAILike\n",
        "\n",
        "llm = OpenAILike(model=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\", api_base=BASE_URL, api_key=API_KEY)\n",
        "# response = llm.complete(\"Hello World!\")\n",
        "# print(str(response))"
      ],
      "metadata": {
        "id": "7aa9QEniUESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en\n",
        "# embed_model = HuggingFaceEmbedding()\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "smXPe4O4RhY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Loading text files in root\n",
        "documents = SimpleDirectoryReader(\".\").load_data()\n",
        "\n",
        "Settings.chunk_size = 100\n",
        "Settings.chunk_overlap = 20\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "rag_application = index.as_query_engine(similarity_top_k=5, llm=None)"
      ],
      "metadata": {
        "id": "WrHVVIuARMSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An example input to your RAG application\n",
        "user_input = \"What are Wells Fargo's policies on lobbying and pay-to-play laws?\"\n",
        "\n",
        "# LlamaIndex returns a response object that contains\n",
        "# both the output string and retrieved nodes\n",
        "response_object = rag_application.query(user_input)\n",
        "\n",
        "# Process the response object to get the output string\n",
        "# and retrieved nodes\n",
        "if response_object is not None:\n",
        "    actual_output = response_object.response\n",
        "    retrieval_context = [node.get_content() for node in response_object.source_nodes]\n"
      ],
      "metadata": {
        "id": "N7HNYh0oRZr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_output"
      ],
      "metadata": {
        "id": "p5-lpDMqpbOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_context"
      ],
      "metadata": {
        "id": "IOrYjjZpqICd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-YKFMxs1D5Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "import tiktoken\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from openai import OpenAI\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import QueryResponseEvaluator\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    stream=sys.stdout\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class EvalConfig:\n",
        "    \"\"\"Configuration for the evaluation process.\"\"\"\n",
        "    # File and model paths\n",
        "    input_file: str\n",
        "    model_path: str\n",
        "    embed_model_name: str = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "    # API settings\n",
        "    api_base: str = BASE_URL\n",
        "    api_key: str = API_KEY\n",
        "\n",
        "    # Chunking settings\n",
        "    chunk_size: int = 50\n",
        "    chunk_overlap: int = 20\n",
        "\n",
        "    # Query generation settings\n",
        "    query_temperature: float = 0.2\n",
        "    queries_per_chunk: int = 1\n",
        "\n",
        "    # LLM settings\n",
        "    max_input_tokens: int = 1152  # Default for a smaller model\n",
        "    token_buffer: int = 100  # Conservative buffer\n",
        "\n",
        "    # Evaluation settings\n",
        "    top_k: int = 5\n",
        "    batch_size: int = 10\n",
        "\n",
        "    # Debug settings\n",
        "    debug: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization.\"\"\"\n",
        "        if self.max_input_tokens <= self.token_buffer:\n",
        "            raise ValueError(\"max_input_tokens must be greater than token_buffer\")\n",
        "        if self.queries_per_chunk < 1:\n",
        "            raise ValueError(\"queries_per_chunk must be at least 1\")\n",
        "\n",
        "class ProcessingError(Exception):\n",
        "    \"\"\"Custom error for processing failures.\"\"\"\n",
        "    pass\n",
        "\n",
        "class RetrievalEvaluator:\n",
        "    def __init__(self, config: EvalConfig):\n",
        "        self.config = config\n",
        "        self.stats = {\n",
        "            'skipped_chunks': 0,\n",
        "            'processed_chunks': 0,\n",
        "            'failed_queries': 0,\n",
        "            'successful_queries': 0,\n",
        "            'errors': [],\n",
        "            'token_stats': {\n",
        "                'max_seen': 0,\n",
        "                'average': 0,\n",
        "                'total_tokens': 0\n",
        "            }\n",
        "        }\n",
        "        self.setup_components()\n",
        "\n",
        "    def setup_components(self):\n",
        "        \"\"\"Initialize all necessary components with robust error checking.\"\"\"\n",
        "        try:\n",
        "            # Setup encoding for token counting\n",
        "            self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "            # Calculate effective token limits\n",
        "            self.effective_max_tokens = int(self.config.max_input_tokens * 0.8)  # 20% safety margin\n",
        "\n",
        "            # Setup base prompt template\n",
        "            self.system_prompt = \"\"\"You are an assistant helping to evaluate an information retrieval system by generating realistic search queries.\n",
        "            For each text chunk provided, generate a specific query (under 10 words) that would be used to retrieve that chunk from a larger document.\n",
        "            Make the query specific enough to target the chunk's key information but natural sounding, as if a Wells Fargo employee was searching the Employee Code of Conduct.\n",
        "            Only output the query itself - no other text, explanation or commentary.\n",
        "            The query should directly relate to the main topic or requirement discussed in the chunk.\n",
        "            Do not include double quotes.\n",
        "            \"\"\"\n",
        "\n",
        "            self.user_prompt_template = \"\"\"Text chunk: {chunk}\n",
        "\n",
        "            Query: \"\"\"\n",
        "\n",
        "            # Calculate and validate base prompt tokens\n",
        "            self.base_prompt_tokens = self.count_tokens(self.system_prompt) + self.count_tokens(self.user_prompt_template)\n",
        "            self.validate_prompt_template()\n",
        "\n",
        "            # Setup components\n",
        "            self.embed_model = HuggingFaceEmbedding(\n",
        "                model_name=self.config.embed_model_name\n",
        "            )\n",
        "\n",
        "            self.client = OpenAI(\n",
        "                base_url=self.config.api_base,\n",
        "                api_key=self.config.api_key\n",
        "            )\n",
        "\n",
        "            self.node_parser = SimpleNodeParser.from_defaults(\n",
        "                chunk_size=self.config.chunk_size,\n",
        "                chunk_overlap=self.config.chunk_overlap\n",
        "            )\n",
        "\n",
        "            # Configure global settings\n",
        "            # Settings.llm = self.llm\n",
        "            Settings.embed_model = self.embed_model\n",
        "            Settings.node_parser = self.node_parser\n",
        "\n",
        "            logger.info(f\"\"\"\n",
        "            Initialization complete:\n",
        "            - Base prompt tokens: {self.base_prompt_tokens}\n",
        "            - Effective max tokens: {self.effective_max_tokens}\n",
        "            - Available tokens for content: {self.effective_max_tokens - self.base_prompt_tokens - self.config.token_buffer}\n",
        "            \"\"\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize components: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def validate_prompt_template(self):\n",
        "        \"\"\"Validate that the prompt template isn't too large.\"\"\"\n",
        "        # Count tokens for both prompts\n",
        "        system_tokens = self.count_tokens(self.system_prompt)\n",
        "        user_tokens = self.count_tokens(self.user_prompt_template)\n",
        "        total_tokens = system_tokens + user_tokens\n",
        "\n",
        "        if total_tokens > self.effective_max_tokens * 0.3:\n",
        "            raise ValueError(f\"Combined prompts too large: {total_tokens} tokens\")\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
        "        try:\n",
        "            return len(self.tokenizer.encode(text))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Token counting failed: {str(e)}\")\n",
        "            self.track_error(\"token_count_error\", {\"text_length\": len(text), \"error\": str(e)})\n",
        "            return float('inf')  # Conservative approach - treat as too long\n",
        "\n",
        "    def track_error(self, error_type: str, details: Dict[str, Any]):\n",
        "        \"\"\"Track detailed error information.\"\"\"\n",
        "        error_info = {\n",
        "            'type': error_type,\n",
        "            'timestamp': time.time(),\n",
        "            'details': details,\n",
        "            'traceback': traceback.format_exc()\n",
        "        }\n",
        "        self.stats['errors'].append(error_info)\n",
        "\n",
        "    def debug_token_counts(self, chunk: str):\n",
        "        \"\"\"Debug token counting for a chunk.\"\"\"\n",
        "        if self.config.debug:\n",
        "            prompt = self.user_prompt_template.format(chunk=chunk)\n",
        "            logger.debug(f\"\"\"\n",
        "                Debug token counts:\n",
        "                - System prompt: {self.count_tokens(self.system_prompt)}\n",
        "                - User prompt template: {self.count_tokens(self.user_prompt_template)}\n",
        "                - Chunk: {self.count_tokens(chunk)}\n",
        "                - Full user prompt: {self.count_tokens(prompt)}\n",
        "                - Buffer: {self.config.token_buffer}\n",
        "                - Max allowed: {self.effective_max_tokens}\n",
        "            \"\"\")\n",
        "\n",
        "    def is_chunk_processable(self, chunk: str) -> bool:\n",
        "        \"\"\"Check if chunk can be processed within token limits.\"\"\"\n",
        "        chunk_tokens = self.count_tokens(chunk)\n",
        "        total_tokens = self.base_prompt_tokens + chunk_tokens\n",
        "\n",
        "        # Update token statistics\n",
        "        self.stats['token_stats']['max_seen'] = max(\n",
        "            self.stats['token_stats']['max_seen'],\n",
        "            total_tokens\n",
        "        )\n",
        "        self.stats['token_stats']['total_tokens'] += total_tokens\n",
        "\n",
        "        # Add extra buffer for message formatting\n",
        "        estimated_total = total_tokens + self.config.token_buffer\n",
        "\n",
        "        is_processable = estimated_total <= self.effective_max_tokens\n",
        "\n",
        "        if not is_processable:\n",
        "            logger.info(f\"\"\"\n",
        "                Chunk exceeds token limits:\n",
        "                - Chunk tokens: {chunk_tokens}\n",
        "                - Base prompt tokens: {self.base_prompt_tokens}\n",
        "                - Estimated total: {estimated_total}\n",
        "                - Limit: {self.effective_max_tokens}\n",
        "                - First 100 chars: {chunk[:100]}...\n",
        "            \"\"\")\n",
        "\n",
        "        return is_processable\n",
        "\n",
        "    def process_large_chunk(self, chunk: str) -> List[str]:\n",
        "        \"\"\"Handle chunks that are too large by taking a subset.\"\"\"\n",
        "        if not self.is_chunk_processable(chunk):\n",
        "            # Take first N tokens that fit within limits\n",
        "            tokens = self.tokenizer.encode(chunk)\n",
        "            safe_token_limit = self.effective_max_tokens - self.base_prompt_tokens - self.config.token_buffer\n",
        "            truncated_tokens = tokens[:safe_token_limit]\n",
        "            truncated_chunk = self.tokenizer.decode(truncated_tokens)\n",
        "\n",
        "            logger.info(f\"Truncated chunk from {len(tokens)} to {len(truncated_tokens)} tokens\")\n",
        "\n",
        "            return self.generate_queries(truncated_chunk)\n",
        "        return self.generate_queries(chunk)\n",
        "\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(3),\n",
        "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "        retry=retry_if_exception_type(Exception)\n",
        "    )\n",
        "    def generate_query_with_retry(self, chunk: str) -> Optional[str]:\n",
        "        \"\"\"Generate a query with retry logic.\"\"\"\n",
        "        try:\n",
        "            if not self.is_chunk_processable(chunk):\n",
        "                return None\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.config.model_path,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": self.user_prompt_template.format(chunk=chunk)}\n",
        "                ],\n",
        "                temperature=self.config.query_temperature\n",
        "            )\n",
        "            self.stats['successful_queries'] += 1\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to generate query: {str(e)}\")\n",
        "            self.stats['failed_queries'] += 1\n",
        "            self.track_error(\"query_generation_error\", {\n",
        "                \"chunk_length\": len(chunk),\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "            return None\n",
        "\n",
        "    def generate_queries(self, chunk: str) -> List[str]:\n",
        "        \"\"\"Generate queries for a given chunk.\"\"\"\n",
        "        self.debug_token_counts(chunk)\n",
        "\n",
        "        if not self.is_chunk_processable(chunk):\n",
        "            self.stats['skipped_chunks'] += 1\n",
        "            logger.warning(f\"\"\"\n",
        "                Skipping chunk due to length:\n",
        "                - Characters: {len(chunk)}\n",
        "                - Tokens: {self.count_tokens(chunk)}\n",
        "            \"\"\")\n",
        "            return []\n",
        "\n",
        "        self.stats['processed_chunks'] += 1\n",
        "        queries = []\n",
        "        for _ in range(self.config.queries_per_chunk):\n",
        "            query = self.generate_query_with_retry(chunk)\n",
        "            if query:\n",
        "                queries.append(query)\n",
        "\n",
        "        return queries\n",
        "\n",
        "    def load_and_index_document(self) -> Tuple[VectorStoreIndex, List[Document]]:\n",
        "        \"\"\"Load document and create index.\"\"\"\n",
        "        logger.info(\"Loading and indexing document...\")\n",
        "\n",
        "        try:\n",
        "            with open(self.config.input_file, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            documents = [Document(text=text)]\n",
        "            nodes = self.node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "            # Create index with local settings\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                embed_model=self.embed_model,\n",
        "                transformations=[self.node_parser]\n",
        "            )\n",
        "\n",
        "            return index, nodes\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load and index document: {str(e)}\")\n",
        "            self.track_error(\"indexing_error\", {\"error\": str(e)})\n",
        "            raise\n",
        "\n",
        "    def evaluate_retrieval(self, index: VectorStoreIndex, nodes: List[Document]) -> Dict:\n",
        "        \"\"\"Evaluate retrieval performance.\"\"\"\n",
        "        logger.info(\"Starting retrieval evaluation...\")\n",
        "\n",
        "        query_engine = index.as_query_engine(\n",
        "            llm=None,\n",
        "            similarity_top_k=self.config.top_k\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            'hit_at_k': [],\n",
        "            'queries': [],\n",
        "            'original_chunks': [],\n",
        "            'retrieval_times': [],\n",
        "            'skipped_chunks': [],\n",
        "            'stats': self.stats\n",
        "        }\n",
        "\n",
        "        for node in tqdm(nodes, desc=\"Evaluating chunks\"):\n",
        "            # Try to process the chunk, potentially with truncation\n",
        "            queries = self.process_large_chunk(node.text)\n",
        "\n",
        "            if not queries:\n",
        "                results['skipped_chunks'].append({\n",
        "                    'chunk': node.text[:200] + \"...\",  # Only store preview\n",
        "                    'reason': 'Token limit exceeded or query generation failed',\n",
        "                    'token_count': self.count_tokens(node.text)\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            for query in queries:\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    response = query_engine.query(query)\n",
        "                    retrieval_time = time.time() - start_time\n",
        "\n",
        "                    retrieved_texts = [n.node.text for n in response.source_nodes]\n",
        "                    is_found = node.text in retrieved_texts\n",
        "\n",
        "                    results['hit_at_k'].append(int(is_found))\n",
        "                    results['queries'].append(query)\n",
        "                    results['original_chunks'].append(node.text)\n",
        "                    results['retrieval_times'].append(retrieval_time)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error during retrieval: {str(e)}\")\n",
        "                    self.track_error(\"retrieval_error\", {\n",
        "                        \"query\": query,\n",
        "                        \"error\": str(e)\n",
        "                    })\n",
        "\n",
        "        # Update average token stats\n",
        "        if self.stats['processed_chunks'] > 0:\n",
        "            self.stats['token_stats']['average'] = (\n",
        "                self.stats['token_stats']['total_tokens'] /\n",
        "                self.stats['processed_chunks']\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_results(self, results: Dict):\n",
        "        \"\"\"Create visualizations of the evaluation results.\"\"\"\n",
        "        if not results['hit_at_k']:\n",
        "            logger.warning(\"No results to visualize - all chunks may have been skipped\")\n",
        "            return\n",
        "\n",
        "        hit = np.mean(results['hit_at_k'])\n",
        "        avg_time = np.mean(results['retrieval_times'])\n",
        "\n",
        "        print(\"\\nEvaluation Statistics:\")\n",
        "        print(f\"Overall Hit@{self.config.top_k}: {hit:.3f}\")\n",
        "        print(f\"Average retrieval time: {avg_time:.3f} seconds\")\n",
        "\n",
        "        print(f\"\\nProcessing Statistics:\")\n",
        "        print(f\"Processed chunks: {self.stats['processed_chunks']}\")\n",
        "        print(f\"Skipped chunks: {self.stats['skipped_chunks']}\")\n",
        "        print(f\"Successful queries: {self.stats['successful_queries']}\")\n",
        "        print(f\"Failed queries: {self.stats['failed_queries']}\")\n",
        "\n",
        "        print(f\"\\nToken Statistics:\")\n",
        "        print(f\"Max tokens seen: {self.stats['token_stats']['max_seen']}\")\n",
        "        print(f\"Average tokens: {self.stats['token_stats']['average']:.1f}\")\n",
        "\n",
        "        if self.stats['errors']:\n",
        "            print(f\"\\nEncountered {len(self.stats['errors'])} errors\")\n",
        "\n",
        "        # Plot timing distribution\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(results['retrieval_times'], bins=30)\n",
        "        plt.title('Distribution of Retrieval Times')\n",
        "        plt.xlabel('Time (seconds)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.show()\n",
        "\n",
        "    def run_evaluation(self):\n",
        "        \"\"\"Run the complete evaluation pipeline.\"\"\"\n",
        "        # Load and index document\n",
        "        index, nodes = self.load_and_index_document()\n",
        "\n",
        "        # Run evaluation\n",
        "        results = self.evaluate_retrieval(index, nodes)\n",
        "\n",
        "        # Visualize results\n",
        "        self.visualize_results(results)\n",
        "\n",
        "        # Save results\n",
        "        output_path = Path(self.config.input_file).with_suffix('.eval_results.json')\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "3viLELyIimSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = EvalConfig(\n",
        "    input_file=\"text.txt\",\n",
        "    model_path=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_input_tokens=1000  # Set based on your model's context window\n",
        ")\n",
        "\n",
        "evaluator = RetrievalEvaluator(config)\n",
        "results = evaluator.run_evaluation()"
      ],
      "metadata": {
        "id": "0hcThQpMiyQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def process_json_to_csvs(json_data, output_dir='output'):\n",
        "    \"\"\"\n",
        "    Process JSON data and create multiple CSV files for different components.\n",
        "    Now includes original chunks and ranking information in hit_queries.csv.\n",
        "\n",
        "    Parameters:\n",
        "    json_data (dict): Parsed JSON data\n",
        "    output_dir (str): Directory to save CSV files\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 1. Create hit_queries.csv with enhanced information\n",
        "    hit_data = pd.DataFrame({\n",
        "        'query': json_data['queries'],\n",
        "        'original_chunk': json_data['original_chunks'],\n",
        "        'hit_score': json_data['hit_at_k'],\n",
        "        'original_chunk_found': [bool(score) for score in json_data['hit_at_k']],\n",
        "        'rank': ['1' if score == 1 else 'Not in top k' if score == 0 else 'Unknown'\n",
        "                for score in json_data['hit_at_k']],\n",
        "        'retrieval_time': json_data['retrieval_times']\n",
        "    })\n",
        "    hit_data.to_csv(f'{output_dir}/hit_queries.csv', index=False)\n",
        "\n",
        "    # 2. Create original_chunks.csv\n",
        "    chunks_data = pd.DataFrame({\n",
        "        'chunk_id': range(len(json_data['original_chunks'])),\n",
        "        'content': json_data['original_chunks']\n",
        "    })\n",
        "    chunks_data.to_csv(f'{output_dir}/original_chunks.csv', index=False)\n",
        "\n",
        "    # 3. Create retrieval_times.csv\n",
        "    retrieval_data = pd.DataFrame({\n",
        "        'chunk_id': range(len(json_data['retrieval_times'])),\n",
        "        'retrieval_time': json_data['retrieval_times']\n",
        "    })\n",
        "    retrieval_data.to_csv(f'{output_dir}/retrieval_times.csv', index=False)\n",
        "\n",
        "    # 4. Create stats.csv - flattening the stats dictionary\n",
        "    stats_dict = json_data['stats']\n",
        "    # Handle nested token_stats\n",
        "    token_stats = stats_dict.pop('token_stats')\n",
        "    stats_dict.update({f'token_{k}': v for k, v in token_stats.items()})\n",
        "    # Convert errors list to string to store in CSV\n",
        "    stats_dict['errors'] = ','.join(map(str, stats_dict['errors']))\n",
        "\n",
        "    stats_df = pd.DataFrame([stats_dict])\n",
        "    stats_df.to_csv(f'{output_dir}/stats.csv', index=False)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(f\"Total queries processed: {len(hit_data)}\")\n",
        "    print(f\"Queries where original chunk was found: {sum(hit_data['original_chunk_found'])}\")\n",
        "    print(f\"Average retrieval time: {hit_data['retrieval_time'].mean():.3f} seconds\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to read JSON from file and process it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the JSON data from the file\n",
        "        with open('text.eval_results.json', 'r', encoding='utf-8') as file:\n",
        "            json_data = json.load(file)\n",
        "\n",
        "        # Process the JSON data and create CSV files\n",
        "        process_json_to_csvs(json_data)\n",
        "\n",
        "        print(\"\\nCSV files have been created successfully in the 'output' directory:\")\n",
        "        print(\"1. hit_queries.csv - Contains queries, original chunks, and retrieval metrics\")\n",
        "        print(\"2. original_chunks.csv - Contains the original text chunks\")\n",
        "        print(\"3. retrieval_times.csv - Contains retrieval times for each chunk\")\n",
        "        print(\"4. stats.csv - Contains processing statistics\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: text.eval_results.json file not found\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: Invalid JSON format in the input file\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2ctEWM-NtoMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}