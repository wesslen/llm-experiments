{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj890e9CL5/VSOUwe2oj3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/loadtest/openai_compatible_endpoint_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXX1tAHR5aDD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('DSBA_LLAMA3_KEY')\n",
        "custom_base_url = userdata.get('MODAL_BASE_URL')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain<=0.3.0 langchain-community"
      ],
      "metadata": {
        "id": "8WncZCXI5c8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import time\n",
        "import statistics\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Literal, Union\n",
        "import numpy as np\n",
        "import json\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import httpx\n",
        "from datetime import datetime\n",
        "\n",
        "# For LangChain 0.2.0\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import openai"
      ],
      "metadata": {
        "id": "2Nmbt_HH5apf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TestConfig:\n",
        "    model_name: str\n",
        "    base_url: str\n",
        "    api_key: str\n",
        "    client_type: Literal[\"requests\", \"openai\", \"langchain\"]\n",
        "    system_prompt: Optional[str] = None\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 1.0\n",
        "    max_tokens: int = 1000\n",
        "    verify_ssl: bool = True\n",
        "    output_path: Optional[str] = None\n",
        "\n",
        "# Cell 3: LoadTester Class\n",
        "class LoadTester:\n",
        "    def __init__(self, config: TestConfig):\n",
        "        self.config = config\n",
        "        self.setup_client()\n",
        "        self.results = []\n",
        "        if self.config.output_path:\n",
        "            os.makedirs(self.config.output_path, exist_ok=True)\n",
        "\n",
        "    def setup_client(self):\n",
        "        if self.config.client_type == \"openai\":\n",
        "            openai.api_key = self.config.api_key\n",
        "            openai.api_base = self.config.base_url\n",
        "        elif self.config.client_type == \"langchain\":\n",
        "            # Updated for LangChain 0.2.0\n",
        "            self.client = ChatOpenAI(\n",
        "                model_name=self.config.model_name,\n",
        "                openai_api_key=self.config.api_key,\n",
        "                openai_api_base=self.config.base_url,\n",
        "                temperature=self.config.temperature,\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                streaming=True\n",
        "            )\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(text) // 4  # Simple approximation\n",
        "\n",
        "    def _make_request(self, prompt: str) -> dict:\n",
        "        start_time = time.time()\n",
        "        first_token_time = None\n",
        "        try:\n",
        "            if self.config.client_type == \"openai\":\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": self.config.system_prompt or \"\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "\n",
        "                completion = \"\"\n",
        "                token_times = []\n",
        "\n",
        "                # Updated for older OpenAI API version\n",
        "                for chunk in openai.ChatCompletion.create(\n",
        "                    model=self.config.model_name,\n",
        "                    messages=messages,\n",
        "                    stream=True,\n",
        "                    temperature=self.config.temperature,\n",
        "                    max_tokens=self.config.max_tokens\n",
        "                ):\n",
        "                    if not first_token_time:\n",
        "                        first_token_time = time.time()\n",
        "\n",
        "                    if 'content' in chunk['choices'][0]['delta']:\n",
        "                        token = chunk['choices'][0]['delta']['content']\n",
        "                        completion += token\n",
        "                        token_times.append(time.time())\n",
        "\n",
        "            elif self.config.client_type == \"langchain\":\n",
        "                messages = []\n",
        "                if self.config.system_prompt:\n",
        "                    messages.append(SystemMessage(content=self.config.system_prompt))\n",
        "                messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "                completion = \"\"\n",
        "                token_times = []\n",
        "\n",
        "                # Updated for LangChain 0.2.0\n",
        "                for chunk in self.client.stream(messages):\n",
        "                    if not first_token_time:\n",
        "                        first_token_time = time.time()\n",
        "                    if chunk.content:\n",
        "                        completion += chunk.content\n",
        "                        token_times.append(time.time())\n",
        "\n",
        "            end_time = time.time()\n",
        "            total_latency = end_time - start_time\n",
        "            ttft = first_token_time - start_time if first_token_time else total_latency\n",
        "\n",
        "            output_tokens = self.count_tokens(completion)\n",
        "            avg_throughput = output_tokens / total_latency if total_latency > 0 else 0\n",
        "\n",
        "            # Calculate token generation rates\n",
        "            if len(token_times) > 1:\n",
        "                token_intervals = np.diff(token_times)\n",
        "                instant_throughputs = 1 / token_intervals\n",
        "            else:\n",
        "                instant_throughputs = [avg_throughput]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"total_latency\": total_latency,\n",
        "                \"time_to_first_token\": ttft,\n",
        "                \"avg_throughput\": avg_throughput,\n",
        "                \"peak_throughput\": max(instant_throughputs),\n",
        "                \"min_throughput\": min(instant_throughputs),\n",
        "                \"prompt_length\": len(prompt),\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"completion\": completion\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Request failed: {str(e)}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"prompt_length\": len(prompt),\n",
        "                \"output_tokens\": 0,\n",
        "                \"throughput\": 0\n",
        "            }\n",
        "\n",
        "    def analyze_results(self, results: List[dict]) -> dict:\n",
        "        successful_requests = [r for r in results if r[\"success\"]]\n",
        "        failed_requests = [r for r in results if not r[\"success\"]]\n",
        "\n",
        "        if not successful_requests:\n",
        "            return {\"error\": \"All requests failed\"}\n",
        "\n",
        "        latencies = [r[\"total_latency\"] for r in successful_requests]\n",
        "        ttft = [r[\"time_to_first_token\"] for r in successful_requests]\n",
        "        avg_throughputs = [r[\"avg_throughput\"] for r in successful_requests]\n",
        "        peak_throughputs = [r[\"peak_throughput\"] for r in successful_requests]\n",
        "        min_throughputs = [r.get(\"min_throughput\", r[\"avg_throughput\"]) for r in successful_requests]\n",
        "        output_tokens = [r[\"output_tokens\"] for r in successful_requests]\n",
        "\n",
        "        return {\n",
        "            \"total_requests\": len(results),\n",
        "            \"successful_requests\": len(successful_requests),\n",
        "            \"failed_requests\": len(failed_requests),\n",
        "            \"latency\": {\n",
        "                \"avg\": statistics.mean(latencies),\n",
        "                \"p50\": np.percentile(latencies, 50),\n",
        "                \"p95\": np.percentile(latencies, 95),\n",
        "                \"p99\": np.percentile(latencies, 99),\n",
        "                \"min\": min(latencies),\n",
        "                \"max\": max(latencies)\n",
        "            },\n",
        "            \"time_to_first_token\": {\n",
        "                \"avg\": statistics.mean(ttft),\n",
        "                \"p50\": np.percentile(ttft, 50),\n",
        "                \"p95\": np.percentile(ttft, 95),\n",
        "                \"p99\": np.percentile(ttft, 99),\n",
        "            },\n",
        "            \"throughput\": {\n",
        "                \"avg\": statistics.mean(avg_throughputs),\n",
        "                \"peak\": statistics.mean(peak_throughputs),\n",
        "                \"min\": statistics.mean(min_throughputs),\n",
        "            },\n",
        "            \"output_tokens\": {\n",
        "                \"total\": sum(output_tokens),\n",
        "                \"avg\": statistics.mean(output_tokens),\n",
        "                \"p50\": np.percentile(output_tokens, 50),\n",
        "                \"p95\": np.percentile(output_tokens, 95),\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def run_latency_test(self, prompts: List[str], concurrency: int = 1):\n",
        "        with ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
        "            results = list(executor.map(self._make_request, prompts))\n",
        "        return self.analyze_results(results)\n",
        "\n",
        "    def run_sustained_load_test(self, prompt: str, requests_per_second: float, duration_seconds: int):\n",
        "        start_time = time.time()\n",
        "        results = []\n",
        "\n",
        "        while time.time() - start_time < duration_seconds:\n",
        "            before_request = time.time()\n",
        "            result = self._make_request(prompt)\n",
        "            results.append(result)\n",
        "\n",
        "            elapsed = time.time() - before_request\n",
        "            wait_time = max(0, (1 / requests_per_second) - elapsed)\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        return self.analyze_results(results)"
      ],
      "metadata": {
        "id": "aeCAwFhF5k3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "QDQI2p5I54aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TestConfig(\n",
        "    model_name=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    base_url=custom_base_url,\n",
        "    api_key=api_key,\n",
        "    client_type=\"openai\",\n",
        "    system_prompt=\"You are a helpful AI assistant.\",\n",
        "    verify_ssl=False,\n",
        "    output_path=\"./test_results\"\n",
        ")\n",
        "\n",
        "tester = LoadTester(config)\n",
        "\n",
        "# Run a simple test\n",
        "results = tester.run_latency_test(\n",
        "    prompts=[\"What is artificial intelligence?\"],\n",
        "    concurrency=1\n",
        ")\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "id": "2AePJg9E5lWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requests"
      ],
      "metadata": {
        "id": "wzNgOJTL6AaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TestConfig(\n",
        "    model_name=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    base_url=custom_base_url,\n",
        "    api_key=api_key,\n",
        "    client_type=\"requests\",\n",
        "    system_prompt=\"You are a helpful AI assistant.\",\n",
        "    verify_ssl=False,\n",
        "    output_path=\"./test_results\"\n",
        ")\n",
        "\n",
        "tester = LoadTester(config)\n",
        "\n",
        "# Run a simple test\n",
        "results = tester.run_latency_test(\n",
        "    prompts=[\"What is artificial intelligence?\"],\n",
        "    concurrency=1\n",
        ")\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "id": "i6HMAARc57_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain"
      ],
      "metadata": {
        "id": "2gqAt_Il6BlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TestConfig(\n",
        "    model_name=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "    base_url=custom_base_url,\n",
        "    api_key=api_key,\n",
        "    client_type=\"langchain\",\n",
        "    system_prompt=\"You are a helpful AI assistant.\",\n",
        "    verify_ssl=False,\n",
        "    output_path=\"./test_results\"\n",
        ")\n",
        "\n",
        "tester = LoadTester(config)\n",
        "\n",
        "# Run a simple test\n",
        "results = tester.run_latency_test(\n",
        "    prompts=[\"What is artificial intelligence?\"],\n",
        "    concurrency=1\n",
        ")\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "id": "vbLfw1Cg5-1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}