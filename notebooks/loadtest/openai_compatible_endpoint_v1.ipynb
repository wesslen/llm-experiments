{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwvm6tudLcc4nLYBwQoonV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/loadtest/openai_compatible_endpoint_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R84fNa9EgfiG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('DSBA_LLAMA3_KEY')\n",
        "custom_base_url = userdata.get('MODAL_BASE_URL')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --system langchain-community langchain<0.3.0 #llama-index llama-index-llms-openai llama-index-llms-openai-like"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRc5TLCNg0Lo",
        "outputId": "40ba7795-1166-4ed0-bd8c-e3e698d50789"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 0.3.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "import aiohttp\n",
        "import statistics\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Literal, Union\n",
        "import numpy as np\n",
        "import json\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import httpx\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class TestConfig:\n",
        "    model_name: str\n",
        "    base_url: str\n",
        "    api_key: str\n",
        "    client_type: Literal[\"requests\", \"openai\", \"langchain\"]\n",
        "    system_prompt: Optional[str] = None\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 1.0\n",
        "    max_tokens: int = 1000\n",
        "    verify_ssl: bool = True\n",
        "    output_path: Optional[str] = None\n",
        "\n",
        "class LoadTester:\n",
        "    def __init__(self, config: TestConfig):\n",
        "        self.config = config\n",
        "        self.setup_client()\n",
        "        self.results = []\n",
        "        if self.config.output_path:\n",
        "            os.makedirs(self.config.output_path, exist_ok=True)\n",
        "\n",
        "    def setup_client(self):\n",
        "        if self.config.client_type == \"openai\":\n",
        "            openai.api_key = self.config.api_key\n",
        "            openai.base_url = self.config.base_url\n",
        "            openai.http_client = httpx.Client(verify=self.config.verify_ssl)\n",
        "        elif self.config.client_type == \"langchain\":\n",
        "            self.client = ChatOpenAI(\n",
        "                model_name=self.config.model_name,\n",
        "                openai_api_key=self.config.api_key,\n",
        "                base_url=self.config.base_url,\n",
        "                temperature=self.config.temperature,\n",
        "                top_p=self.config.top_p,\n",
        "                max_tokens=self.config.max_tokens,\n",
        "                client=httpx.Client(verify=self.config.verify_ssl),\n",
        "                streaming=True  # Enable streaming\n",
        "            )\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        # Simple approximation: 4 chars ~ 1 token\n",
        "        return len(text) // 4\n",
        "\n",
        "    # async def _make_request(self, prompt: str) -> dict:\n",
        "    #     start_time = time.time()\n",
        "    #     try:\n",
        "    #         if self.config.client_type == \"requests\":\n",
        "    #             async with aiohttp.ClientSession() as session:\n",
        "    #                 async with session.post(\n",
        "    #                     f\"{self.config.base_url}/v1/chat/completions\",\n",
        "    #                     headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n",
        "    #                     json={\n",
        "    #                         \"model\": self.config.model_name,\n",
        "    #                         \"messages\": [\n",
        "    #                             {\"role\": \"system\", \"content\": self.config.system_prompt or \"\"},\n",
        "    #                             {\"role\": \"user\", \"content\": prompt}\n",
        "    #                         ],\n",
        "    #                         \"temperature\": self.config.temperature,\n",
        "    #                         \"top_p\": self.config.top_p,\n",
        "    #                         \"max_tokens\": self.config.max_tokens\n",
        "    #                     },\n",
        "    #                     ssl=self.config.verify_ssl\n",
        "    #                 ) as response:\n",
        "    #                     result = await response.json()\n",
        "    #                     completion = result['choices'][0]['message']['content']\n",
        "\n",
        "    #         elif self.config.client_type == \"openai\":\n",
        "    #             # Fix: Use sync client for OpenAI\n",
        "    #             result = openai.chat.completions.create(\n",
        "    #                 model=self.config.model_name,\n",
        "    #                 messages=[\n",
        "    #                     {\"role\": \"system\", \"content\": self.config.system_prompt or \"\"},\n",
        "    #                     {\"role\": \"user\", \"content\": prompt}\n",
        "    #                 ],\n",
        "    #                 temperature=self.config.temperature,\n",
        "    #                 top_p=self.config.top_p,\n",
        "    #                 max_tokens=min(self.config.max_tokens, 500)  # Reduce max tokens\n",
        "    #             )\n",
        "    #             completion = result.choices[0].message.content\n",
        "\n",
        "    #         elif self.config.client_type == \"langchain\":\n",
        "    #             messages = []\n",
        "    #             if self.config.system_prompt:\n",
        "    #                 messages.append(SystemMessage(content=self.config.system_prompt))\n",
        "    #             messages.append(HumanMessage(content=prompt))\n",
        "    #             result = await self.client.agenerate([messages])\n",
        "    #             completion = result.generations[0][0].text\n",
        "\n",
        "    #         latency = time.time() - start_time\n",
        "    #         output_tokens = self.count_tokens(completion)\n",
        "    #         throughput = output_tokens / latency if latency > 0 else 0\n",
        "\n",
        "    #         return {\n",
        "    #             \"success\": True,\n",
        "    #             \"latency\": latency,\n",
        "    #             \"prompt_length\": len(prompt),\n",
        "    #             \"output_tokens\": output_tokens,\n",
        "    #             \"throughput\": throughput,\n",
        "    #             \"completion\": completion\n",
        "    #         }\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         logging.error(f\"Request failed: {str(e)}\")\n",
        "    #         return {\n",
        "    #             \"success\": False,\n",
        "    #             \"error\": str(e),\n",
        "    #             \"prompt_length\": len(prompt),\n",
        "    #             \"output_tokens\": 0,\n",
        "    #             \"throughput\": 0\n",
        "    #         }\n",
        "    async def _make_request(self, prompt: str) -> dict:\n",
        "        start_time = time.time()\n",
        "        first_token_time = None\n",
        "        try:\n",
        "            if self.config.client_type == \"requests\":\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    async with session.post(\n",
        "                        f\"{self.config.base_url}/v1/chat/completions\",\n",
        "                        headers={\"Authorization\": f\"Bearer {self.config.api_key}\"},\n",
        "                        json={\n",
        "                            \"model\": self.config.model_name,\n",
        "                            \"messages\": [\n",
        "                                {\"role\": \"system\", \"content\": self.config.system_prompt or \"\"},\n",
        "                                {\"role\": \"user\", \"content\": prompt}\n",
        "                            ],\n",
        "                            \"temperature\": self.config.temperature,\n",
        "                            \"top_p\": self.config.top_p,\n",
        "                            \"max_tokens\": self.config.max_tokens,\n",
        "                            \"stream\": True\n",
        "                        },\n",
        "                        ssl=self.config.verify_ssl\n",
        "                    ) as response:\n",
        "                        completion = \"\"\n",
        "                        token_times = []\n",
        "\n",
        "                        async for line in response.content:\n",
        "                            if line:\n",
        "                                if not first_token_time:\n",
        "                                    first_token_time = time.time()\n",
        "\n",
        "                                json_response = json.loads(line.decode('utf-8').split('data: ')[1])\n",
        "                                if json_response['choices'][0]['delta'].get('content'):\n",
        "                                    completion += json_response['choices'][0]['delta']['content']\n",
        "                                    token_times.append(time.time())\n",
        "\n",
        "            elif self.config.client_type == \"openai\":\n",
        "                stream = openai.chat.completions.create(\n",
        "                    model=self.config.model_name,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": self.config.system_prompt or \"\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=self.config.temperature,\n",
        "                    stream=True,\n",
        "                    max_tokens=min(self.config.max_tokens, 500)\n",
        "                )\n",
        "\n",
        "                completion = \"\"\n",
        "                token_times = []\n",
        "\n",
        "                for chunk in stream:\n",
        "                    if not first_token_time:\n",
        "                        first_token_time = time.time()\n",
        "\n",
        "                    if chunk.choices[0].delta.content:\n",
        "                        completion += chunk.choices[0].delta.content\n",
        "                        token_times.append(time.time())\n",
        "\n",
        "            elif self.config.client_type == \"langchain\":\n",
        "                messages = []\n",
        "                if self.config.system_prompt:\n",
        "                    messages.append(SystemMessage(content=self.config.system_prompt))\n",
        "                messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "                completion = \"\"\n",
        "                token_times = []\n",
        "\n",
        "                async for chunk in self.client.astream([messages]):\n",
        "                    if not first_token_time:\n",
        "                        first_token_time = time.time()\n",
        "                    if chunk.content:\n",
        "                        completion += chunk.content\n",
        "                        token_times.append(time.time())\n",
        "\n",
        "            end_time = time.time()\n",
        "            total_latency = end_time - start_time\n",
        "            ttft = first_token_time - start_time if first_token_time else total_latency\n",
        "\n",
        "            output_tokens = self.count_tokens(completion)\n",
        "            avg_throughput = output_tokens / total_latency if total_latency > 0 else 0\n",
        "\n",
        "            # Calculate token generation rate over time\n",
        "            if len(token_times) > 1:\n",
        "                token_intervals = np.diff(token_times)\n",
        "                instant_throughputs = 1 / token_intervals  # tokens per second\n",
        "            else:\n",
        "                instant_throughputs = [avg_throughput]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"total_latency\": total_latency,\n",
        "                \"time_to_first_token\": ttft,\n",
        "                \"avg_throughput\": avg_throughput,\n",
        "                \"peak_throughput\": max(instant_throughputs),\n",
        "                \"min_throughput\": min(instant_throughputs),\n",
        "                \"prompt_length\": len(prompt),\n",
        "                \"output_tokens\": output_tokens,\n",
        "                \"completion\": completion\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Request failed: {str(e)}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"prompt_length\": len(prompt),\n",
        "                \"output_tokens\": 0,\n",
        "                \"throughput\": 0\n",
        "            }\n",
        "\n",
        "    def save_results(self, results: dict, test_name: str):\n",
        "        if self.config.output_path:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"{test_name}_{timestamp}.json\"\n",
        "            filepath = os.path.join(self.config.output_path, filename)\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "\n",
        "    def analyze_results(self, results: List[dict]) -> dict:\n",
        "        successful_requests = [r for r in results if r[\"success\"]]\n",
        "        failed_requests = [r for r in results if not r[\"success\"]]\n",
        "\n",
        "        if not successful_requests:\n",
        "            return {\"error\": \"All requests failed\"}\n",
        "\n",
        "        # Updated key names to match _make_request\n",
        "        latencies = [r[\"total_latency\"] for r in successful_requests]\n",
        "        ttft = [r[\"time_to_first_token\"] for r in successful_requests]\n",
        "        avg_throughputs = [r[\"avg_throughput\"] for r in successful_requests]\n",
        "        peak_throughputs = [r[\"peak_throughput\"] for r in successful_requests]\n",
        "        min_throughputs = [r.get(\"min_throughput\", r[\"avg_throughput\"]) for r in successful_requests]\n",
        "        output_tokens = [r[\"output_tokens\"] for r in successful_requests]\n",
        "\n",
        "        analysis = {\n",
        "            \"total_requests\": len(results),\n",
        "            \"successful_requests\": len(successful_requests),\n",
        "            \"failed_requests\": len(failed_requests),\n",
        "            \"latency\": {\n",
        "                \"avg\": statistics.mean(latencies),\n",
        "                \"p50\": np.percentile(latencies, 50),\n",
        "                \"p95\": np.percentile(latencies, 95),\n",
        "                \"p99\": np.percentile(latencies, 99),\n",
        "                \"min\": min(latencies),\n",
        "                \"max\": max(latencies)\n",
        "            },\n",
        "            \"time_to_first_token\": {\n",
        "                \"avg\": statistics.mean(ttft),\n",
        "                \"p50\": np.percentile(ttft, 50),\n",
        "                \"p95\": np.percentile(ttft, 95),\n",
        "                \"p99\": np.percentile(ttft, 99),\n",
        "                \"min\": min(ttft),\n",
        "                \"max\": max(ttft)\n",
        "            },\n",
        "            \"throughput\": {\n",
        "                \"avg\": statistics.mean(avg_throughputs),\n",
        "                \"peak\": statistics.mean(peak_throughputs),\n",
        "                \"min\": statistics.mean(min_throughputs),\n",
        "                \"p50\": np.percentile(avg_throughputs, 50),\n",
        "                \"p95\": np.percentile(avg_throughputs, 95),\n",
        "                \"p99\": np.percentile(avg_throughputs, 99)\n",
        "            },\n",
        "            \"output_tokens\": {\n",
        "                \"total\": sum(output_tokens),\n",
        "                \"avg\": statistics.mean(output_tokens),\n",
        "                \"p50\": np.percentile(output_tokens, 50),\n",
        "                \"p95\": np.percentile(output_tokens, 95),\n",
        "                \"p99\": np.percentile(output_tokens, 99)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    async def run_latency_test(self, prompts: List[str], concurrency: int = 1):\n",
        "        async def _batch_requests(batch: List[str]):\n",
        "            tasks = [self._make_request(prompt) for prompt in batch]\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "        results = []\n",
        "        for i in range(0, len(prompts), concurrency):\n",
        "            batch = prompts[i:i + concurrency]\n",
        "            batch_results = await _batch_requests(batch)\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        analysis = self.analyze_results(results)\n",
        "        self.save_results(analysis, \"latency_test\")\n",
        "        return analysis\n",
        "\n",
        "    async def run_sustained_load_test(self,\n",
        "                                    prompt: str,\n",
        "                                    requests_per_second: float,\n",
        "                                    duration_seconds: int):\n",
        "        start_time = time.time()\n",
        "        results = []\n",
        "\n",
        "        while time.time() - start_time < duration_seconds:\n",
        "            before_request = time.time()\n",
        "            result = await self._make_request(prompt)\n",
        "            results.append(result)\n",
        "\n",
        "            elapsed = time.time() - before_request\n",
        "            wait_time = max(0, (1 / requests_per_second) - elapsed)\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "        analysis = self.analyze_results(results)\n",
        "        self.save_results(analysis, \"sustained_test\")\n",
        "        return analysis\n",
        "\n",
        "    def generate_variable_length_prompts(self,\n",
        "                                       base_prompt: str,\n",
        "                                       n_prompts: int,\n",
        "                                       min_length: int = 100,\n",
        "                                       max_length: int = 1000) -> List[str]:\n",
        "        lengths = np.linspace(min_length, max_length, n_prompts, dtype=int)\n",
        "        prompts = []\n",
        "\n",
        "        for length in lengths:\n",
        "            padding_length = max(0, length - len(base_prompt))\n",
        "            padding = \"X\" * padding_length\n",
        "            prompts.append(base_prompt + padding)\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "l5NZk0vFgf2Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def run_notebook_tests():\n",
        "    config = TestConfig(\n",
        "        model_name=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "        base_url=custom_base_url,\n",
        "        api_key=api_key,\n",
        "        client_type=\"openai\",\n",
        "        system_prompt=\"You are a helpful AI assistant.\",\n",
        "        verify_ssl=False,\n",
        "        output_path=\"./test_results\"\n",
        "    )\n",
        "\n",
        "    tester = LoadTester(config)\n",
        "    results = {}\n",
        "\n",
        "    # Basic test\n",
        "    results['basic'] = await tester.run_latency_test(\n",
        "        prompts=[\"What is artificial intelligence?\"],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # Long prompt test\n",
        "    long_prompt = \"Explain the complete history of artificial intelligence, \" * 50\n",
        "    results['long_prompt'] = await tester.run_latency_test(\n",
        "        prompts=[long_prompt],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # High temperature test\n",
        "    config.temperature = 0.9\n",
        "    tester = LoadTester(config)\n",
        "    results['high_temp'] = await tester.run_latency_test(\n",
        "        prompts=[\"Write a creative story about a robot.\"] * 5,\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # Concurrent requests test\n",
        "    config.temperature = 0.7\n",
        "    tester = LoadTester(config)\n",
        "    results['concurrent'] = await tester.run_latency_test(\n",
        "        prompts=[\"Summarize the benefits of exercise.\"] * 10,\n",
        "        concurrency=5\n",
        "    )\n",
        "\n",
        "    # Sustained load test\n",
        "    results['sustained'] = await tester.run_sustained_load_test(\n",
        "        prompt=\"What are the benefits of meditation?\",\n",
        "        requests_per_second=2,\n",
        "        duration_seconds=30\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run tests\n",
        "results = await run_notebook_tests()\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SzB-jw-obcD",
        "outputId": "ef5c460c-9e87-4d28-bcbb-932d8cb413da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"basic\": {\n",
            "    \"total_requests\": 1,\n",
            "    \"successful_requests\": 1,\n",
            "    \"failed_requests\": 0,\n",
            "    \"latency\": {\n",
            "      \"avg\": 13.740294694900513,\n",
            "      \"p50\": 13.740294694900513,\n",
            "      \"p95\": 13.740294694900513,\n",
            "      \"p99\": 13.740294694900513,\n",
            "      \"min\": 13.740294694900513,\n",
            "      \"max\": 13.740294694900513\n",
            "    },\n",
            "    \"time_to_first_token\": {\n",
            "      \"avg\": 0.3716566562652588,\n",
            "      \"p50\": 0.3716566562652588,\n",
            "      \"p95\": 0.3716566562652588,\n",
            "      \"p99\": 0.3716566562652588,\n",
            "      \"min\": 0.3716566562652588,\n",
            "      \"max\": 0.3716566562652588\n",
            "    },\n",
            "    \"throughput\": {\n",
            "      \"avg\": 32.532053345689654,\n",
            "      \"peak\": 84.83109843658355,\n",
            "      \"min\": 15.076090277453282,\n",
            "      \"p50\": 32.532053345689654,\n",
            "      \"p95\": 32.532053345689654,\n",
            "      \"p99\": 32.532053345689654\n",
            "    },\n",
            "    \"output_tokens\": {\n",
            "      \"total\": 447,\n",
            "      \"avg\": 447,\n",
            "      \"p50\": 447.0,\n",
            "      \"p95\": 447.0,\n",
            "      \"p99\": 447.0\n",
            "    }\n",
            "  },\n",
            "  \"long_prompt\": {\n",
            "    \"total_requests\": 1,\n",
            "    \"successful_requests\": 1,\n",
            "    \"failed_requests\": 0,\n",
            "    \"latency\": {\n",
            "      \"avg\": 0.8624181747436523,\n",
            "      \"p50\": 0.8624181747436523,\n",
            "      \"p95\": 0.8624181747436523,\n",
            "      \"p99\": 0.8624181747436523,\n",
            "      \"min\": 0.8624181747436523,\n",
            "      \"max\": 0.8624181747436523\n",
            "    },\n",
            "    \"time_to_first_token\": {\n",
            "      \"avg\": 0.24393200874328613,\n",
            "      \"p50\": 0.24393200874328613,\n",
            "      \"p95\": 0.24393200874328613,\n",
            "      \"p99\": 0.24393200874328613,\n",
            "      \"min\": 0.24393200874328613,\n",
            "      \"max\": 0.24393200874328613\n",
            "    },\n",
            "    \"throughput\": {\n",
            "      \"avg\": 20.871545298022472,\n",
            "      \"peak\": 69.32275552029618,\n",
            "      \"min\": 22.322595066393465,\n",
            "      \"p50\": 20.871545298022472,\n",
            "      \"p95\": 20.871545298022472,\n",
            "      \"p99\": 20.871545298022472\n",
            "    },\n",
            "    \"output_tokens\": {\n",
            "      \"total\": 18,\n",
            "      \"avg\": 18,\n",
            "      \"p50\": 18.0,\n",
            "      \"p95\": 18.0,\n",
            "      \"p99\": 18.0\n",
            "    }\n",
            "  },\n",
            "  \"high_temp\": {\n",
            "    \"total_requests\": 5,\n",
            "    \"successful_requests\": 5,\n",
            "    \"failed_requests\": 0,\n",
            "    \"latency\": {\n",
            "      \"avg\": 19.425109910964967,\n",
            "      \"p50\": 19.438934564590454,\n",
            "      \"p95\": 19.531659841537476,\n",
            "      \"p99\": 19.531924390792845,\n",
            "      \"min\": 19.261369943618774,\n",
            "      \"max\": 19.53199052810669\n",
            "    },\n",
            "    \"time_to_first_token\": {\n",
            "      \"avg\": 0.1690530776977539,\n",
            "      \"p50\": 0.15269684791564941,\n",
            "      \"p95\": 0.2358316421508789,\n",
            "      \"p99\": 0.2501810836791992,\n",
            "      \"min\": 0.1360766887664795,\n",
            "      \"max\": 0.2537684440612793\n",
            "    },\n",
            "    \"throughput\": {\n",
            "      \"avg\": 30.30137973865704,\n",
            "      \"peak\": 99.13898568947184,\n",
            "      \"min\": 12.91501210462524,\n",
            "      \"p50\": 30.300014542613347,\n",
            "      \"p95\": 30.97866317832448,\n",
            "      \"p99\": 31.059614791484897\n",
            "    },\n",
            "    \"output_tokens\": {\n",
            "      \"total\": 2943,\n",
            "      \"avg\": 588.6,\n",
            "      \"p50\": 589.0,\n",
            "      \"p95\": 604.0,\n",
            "      \"p99\": 606.4\n",
            "    }\n",
            "  },\n",
            "  \"concurrent\": {\n",
            "    \"total_requests\": 10,\n",
            "    \"successful_requests\": 10,\n",
            "    \"failed_requests\": 0,\n",
            "    \"latency\": {\n",
            "      \"avg\": 16.18183581829071,\n",
            "      \"p50\": 16.582199215888977,\n",
            "      \"p95\": 17.6177029132843,\n",
            "      \"p99\": 17.785144510269166,\n",
            "      \"min\": 14.311758041381836,\n",
            "      \"max\": 17.82700490951538\n",
            "    },\n",
            "    \"time_to_first_token\": {\n",
            "      \"avg\": 0.15534045696258544,\n",
            "      \"p50\": 0.1446232795715332,\n",
            "      \"p95\": 0.216316258907318,\n",
            "      \"p99\": 0.2503526759147644,\n",
            "      \"min\": 0.12824106216430664,\n",
            "      \"max\": 0.258861780166626\n",
            "    },\n",
            "    \"throughput\": {\n",
            "      \"avg\": 33.51946971362866,\n",
            "      \"peak\": 824.5055576005188,\n",
            "      \"min\": 12.47344901018784,\n",
            "      \"p50\": 33.551347949457224,\n",
            "      \"p95\": 34.50884924949309,\n",
            "      \"p99\": 34.85338403643861\n",
            "    },\n",
            "    \"output_tokens\": {\n",
            "      \"total\": 5427,\n",
            "      \"avg\": 542.7,\n",
            "      \"p50\": 553.0,\n",
            "      \"p95\": 596.05,\n",
            "      \"p99\": 600.01\n",
            "    }\n",
            "  },\n",
            "  \"sustained\": {\n",
            "    \"total_requests\": 2,\n",
            "    \"successful_requests\": 2,\n",
            "    \"failed_requests\": 0,\n",
            "    \"latency\": {\n",
            "      \"avg\": 18.86541473865509,\n",
            "      \"p50\": 18.86541473865509,\n",
            "      \"p95\": 18.967716872692108,\n",
            "      \"p99\": 18.97681039571762,\n",
            "      \"min\": 18.75174570083618,\n",
            "      \"max\": 18.979083776474\n",
            "    },\n",
            "    \"time_to_first_token\": {\n",
            "      \"avg\": 0.15490007400512695,\n",
            "      \"p50\": 0.15490007400512695,\n",
            "      \"p95\": 0.16692280769348145,\n",
            "      \"p99\": 0.1679914951324463,\n",
            "      \"min\": 0.1415414810180664,\n",
            "      \"max\": 0.1682586669921875\n",
            "    },\n",
            "    \"throughput\": {\n",
            "      \"avg\": 35.03701635720641,\n",
            "      \"peak\": 943.2533774248947,\n",
            "      \"min\": 11.262455981685815,\n",
            "      \"p50\": 35.03701635720641,\n",
            "      \"p95\": 35.13325799734633,\n",
            "      \"p99\": 35.141812809803206\n",
            "    },\n",
            "    \"output_tokens\": {\n",
            "      \"total\": 1322,\n",
            "      \"avg\": 661,\n",
            "      \"p50\": 661.0,\n",
            "      \"p95\": 666.4,\n",
            "      \"p99\": 666.88\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "async def run_comprehensive_tests():\n",
        "    config = TestConfig(\n",
        "        model_name=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
        "        base_url=custom_base_url,\n",
        "        api_key=api_key,\n",
        "        client_type=\"langchain\",\n",
        "        system_prompt=\"You are a helpful AI assistant.\",\n",
        "        verify_ssl=False,\n",
        "        output_path=\"./test_results\"\n",
        "    )\n",
        "\n",
        "    tester = LoadTester(config)\n",
        "    results = {}\n",
        "\n",
        "    # 1. Basic Baseline Test\n",
        "    print(\"\\nRunning Basic Baseline Test...\")\n",
        "    results['baseline'] = await tester.run_latency_test(\n",
        "        prompts=[\"Explain what is machine learning in simple terms.\"],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # 2. System Prompt Impact Test\n",
        "    print(\"\\nRunning System Prompt Impact Test...\")\n",
        "    # Without system prompt\n",
        "    config.system_prompt = None\n",
        "    tester = LoadTester(config)\n",
        "    results['no_system_prompt'] = await tester.run_latency_test(\n",
        "        prompts=[\"Explain what is machine learning in simple terms.\"],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # With detailed system prompt\n",
        "    config.system_prompt = \"\"\"You are an AI assistant with expertise in technical topics.\n",
        "    Always provide detailed, well-structured explanations with examples.\n",
        "    Break down complex concepts into simpler terms.\"\"\"\n",
        "    tester = LoadTester(config)\n",
        "    results['with_system_prompt'] = await tester.run_latency_test(\n",
        "        prompts=[\"Explain what is machine learning in simple terms.\"],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # 3. Variable Length Prompts (Short)\n",
        "    print(\"\\nRunning Short Variable Length Test...\")\n",
        "    base_prompt = \"Explain the concept of machine learning\"\n",
        "    short_prompts = tester.generate_variable_length_prompts(\n",
        "        base_prompt=base_prompt,\n",
        "        n_prompts=5,\n",
        "        min_length=100,\n",
        "        max_length=1000\n",
        "    )\n",
        "    results['short_variable'] = await tester.run_latency_test(\n",
        "        prompts=short_prompts,\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # 4. Variable Length Prompts (Long)\n",
        "    print(\"\\nRunning Long Variable Length Test...\")\n",
        "    long_prompts = tester.generate_variable_length_prompts(\n",
        "        base_prompt=base_prompt,\n",
        "        n_prompts=3,\n",
        "        min_length=1000,\n",
        "        max_length=3000\n",
        "    )\n",
        "    results['long_variable'] = await tester.run_latency_test(\n",
        "        prompts=long_prompts,\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # 5. Chain-of-Thought vs Direct\n",
        "    print(\"\\nRunning Chain-of-Thought Test...\")\n",
        "    cot_prompt = \"\"\"Question: A ball costs $6. A bat costs $12 more than the ball.\n",
        "    How much do the ball and bat cost together?\n",
        "    Let's solve this step by step:\n",
        "    1) First, let's find the cost of the bat\n",
        "    2) Then, add the costs together\"\"\"\n",
        "\n",
        "    direct_prompt = \"A ball costs $6. A bat costs $12 more than the ball. How much do the ball and bat cost together?\"\n",
        "\n",
        "    results['reasoning_comparison'] = await tester.run_latency_test(\n",
        "        prompts=[cot_prompt, direct_prompt],\n",
        "        concurrency=1\n",
        "    )\n",
        "\n",
        "    # 6. Temperature Sweep\n",
        "    print(\"\\nRunning Temperature Sweep Test...\")\n",
        "    temps = [0.0, 0.3, 0.7, 1.0]\n",
        "    temp_results = {}\n",
        "\n",
        "    for temp in temps:\n",
        "        config.temperature = temp\n",
        "        tester = LoadTester(config)\n",
        "        temp_results[f'temp_{temp}'] = await tester.run_latency_test(\n",
        "            prompts=[\"Write a creative story about a robot discovering emotions.\"],\n",
        "            concurrency=1\n",
        "        )\n",
        "    results['temperature_sweep'] = temp_results\n",
        "\n",
        "    # 7. Concurrent Load (Light)\n",
        "    print(\"\\nRunning Light Concurrent Load Test...\")\n",
        "    results['light_concurrent'] = await tester.run_latency_test(\n",
        "        prompts=[\"Explain a complex topic simply.\"] * 3,\n",
        "        concurrency=3\n",
        "    )\n",
        "\n",
        "    # 8. Concurrent Load (Heavy)\n",
        "    print(\"\\nRunning Heavy Concurrent Load Test...\")\n",
        "    results['heavy_concurrent'] = await tester.run_latency_test(\n",
        "        prompts=[\"Explain a complex topic simply.\"] * 10,\n",
        "        concurrency=10\n",
        "    )\n",
        "\n",
        "    # 9. Burst Load\n",
        "    print(\"\\nRunning Burst Load Test...\")\n",
        "    results['burst'] = await tester.run_latency_test(\n",
        "        prompts=[\"Give me a quick explanation.\"] * 5,\n",
        "        concurrency=5\n",
        "    )\n",
        "\n",
        "    # 10. Mixed Workload\n",
        "    print(\"\\nRunning Mixed Workload Test...\")\n",
        "    mixed_prompts = [\n",
        "        \"Short simple response.\",\n",
        "        \"A\" * 1000 + \" Please explain this.\",\n",
        "        \"Write a creative story about robots.\",\n",
        "        \"Explain quantum computing step by step.\",\n",
        "        \"Summarize the theory of relativity briefly.\"\n",
        "    ]\n",
        "    results['mixed_workload'] = await tester.run_latency_test(\n",
        "        prompts=mixed_prompts,\n",
        "        concurrency=2  # Process in small batches\n",
        "    )\n",
        "\n",
        "    # Save all results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    with open(f'comprehensive_results_{timestamp}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run all tests\n",
        "results = await run_comprehensive_tests()\n",
        "\n",
        "# Create summary DataFrame\n",
        "def extract_metrics(result):\n",
        "    if isinstance(result, dict) and 'latency' in result:\n",
        "        return pd.Series({\n",
        "            'avg_latency': result['latency']['avg'],\n",
        "            'ttft': result['time_to_first_token']['avg'],\n",
        "            'avg_throughput': result['throughput']['avg'],\n",
        "            'peak_throughput': result['throughput']['peak'],\n",
        "            'total_tokens': result['output_tokens']['total']\n",
        "        })\n",
        "    return pd.Series()\n",
        "\n",
        "summary_data = {}\n",
        "for test_name, result in results.items():\n",
        "    if test_name != 'temperature_sweep':\n",
        "        summary_data[test_name] = extract_metrics(result)\n",
        "    else:\n",
        "        for temp_name, temp_result in result.items():\n",
        "            summary_data[temp_name] = extract_metrics(temp_result)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data).T\n",
        "print(\"\\nTest Summary:\")\n",
        "print(summary_df.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmQGctPtwuQG",
        "outputId": "fc654572-5314-47ed-cf92-3af72c8df7e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are a helpful AI assistant.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unsupported message type: <class 'list'>\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Basic Baseline Test...\n",
            "\n",
            "Running System Prompt Impact Test...\n",
            "\n",
            "Running Short Variable Length Test...\n",
            "\n",
            "Running Long Variable Length Test...\n",
            "\n",
            "Running Chain-of-Thought Test...\n",
            "\n",
            "Running Temperature Sweep Test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n",
            "ERROR:root:Request failed: Unexpected message type: 'content='You are an AI assistant with expertise in technical topics.\\n    Always provide detailed, well-structured explanations with examples.\\n    Break down complex concepts into simpler terms.' additional_kwargs={} response_metadata={}'. Use one of 'human', 'user', 'ai', 'assistant', 'function', 'tool', or 'system'.\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Light Concurrent Load Test...\n",
            "\n",
            "Running Heavy Concurrent Load Test...\n",
            "\n",
            "Running Burst Load Test...\n",
            "\n",
            "Running Mixed Workload Test...\n",
            "\n",
            "Test Summary:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [baseline, no_system_prompt, with_system_prompt, short_variable, long_variable, reasoning_comparison, temp_0.0, temp_0.3, temp_0.7, temp_1.0, light_concurrent, heavy_concurrent, burst, mixed_workload]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics Overview\n",
        "\n",
        "### Time to First Token (TTFT)\n",
        "* Range: 0.15-0.42 seconds\n",
        "* Notable patterns:\n",
        "  * Baseline test: Highest at 0.42s\n",
        "  * Concurrent loads: Lowest at 0.15-0.16s\n",
        "  * System prompt presence: Minimal impact\n",
        "\n",
        "### Average Latency\n",
        "* Range: 2.35-20.00 seconds\n",
        "* Key observations:\n",
        "  * Temperature tests: Consistent at ~19.5-20.0s\n",
        "  * Burst test: Exceptionally low at 2.35s\n",
        "  * System prompt impact: Increases from 14.21s to 17.40s\n",
        "\n",
        "### Throughput Characteristics\n",
        "* Average throughput: Consistent at 26-34 tokens/sec\n",
        "* Peak throughput variations:\n",
        "  * Highest: Temperature 1.0 (4,593 tokens/sec)\n",
        "  * Lowest: Temperature 0.0 (62.88 tokens/sec)\n",
        "  * System prompt effect: Increases from 117 to 2,438 tokens/sec\n",
        "\n",
        "### Token Generation\n",
        "* Heavy concurrent load: 5,711 tokens\n",
        "* Short variable length test: 3,238 tokens\n",
        "* Baseline tests: ~280-300 tokens\n",
        "* Temperature variations: Consistent at ~585-632 tokens\n",
        "\n",
        "## Key Performance Insights\n",
        "\n",
        "### System Prompt Impact\n",
        "* Token generation: Increases from 448 to 605\n",
        "* Latency: Increases from 14.21s to 17.40s\n",
        "* Peak throughput: Dramatic improvement from 117 to 2,438 tokens/sec\n",
        "\n",
        "### Concurrency Performance\n",
        "* Excellent scaling with concurrent requests\n",
        "* Consistent TTFT under load\n",
        "* Efficient token generation in heavy concurrent scenarios\n",
        "\n",
        "### Temperature Effects\n",
        "* Average throughput: Minimal impact\n",
        "* Peak throughput: Significantly higher at higher temperatures\n",
        "* Token generation: Consistent across temperature settings\n",
        "\n",
        "### Load Pattern Response\n",
        "* Burst handling: Highly efficient (2.35s latency)\n",
        "* Mixed workload: Well-balanced performance\n",
        "* Variable length prompts: Good handling without degradation\n"
      ],
      "metadata": {
        "id": "XRUCGW9w0nLY"
      }
    }
  ]
}