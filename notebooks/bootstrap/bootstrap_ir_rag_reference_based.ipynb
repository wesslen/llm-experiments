{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUnsdQFsxLCz0xQ7ofFyn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/bootstrap/bootstrap_ir_rag_reference_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install --system \"outlines[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnfSHRtph5nX",
        "outputId": "0e4c21b4-de91-4a43-86b2-9975b62c6e84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.10.12 environment at /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 328ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EbZKAbxiVOyY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, List, Optional, Union, Literal\n",
        "from dataclasses import dataclass, asdict\n",
        "from pydantic import BaseModel, ConfigDict, Field\n",
        "from outlines.models.openai import OpenAIConfig\n",
        "from outlines import models, generate\n",
        "import openai\n",
        "from openai import AsyncOpenAI, DefaultHttpxClient\n",
        "import httpx\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from enum import Enum\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from collections import defaultdict\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        ")\n",
        "\n",
        "class QuestionType(str, Enum):\n",
        "    FACTUAL = \"factual\"\n",
        "    PROCEDURAL = \"procedural\"\n",
        "    CONCEPTUAL = \"conceptual\"\n",
        "    COMPARATIVE = \"comparative\"\n",
        "    ANALYTICAL = \"analytical\"\n",
        "\n",
        "@dataclass\n",
        "class QueryMetadata(BaseModel):\n",
        "    \"\"\"Metadata for a generated query\"\"\"\n",
        "    model_config = ConfigDict(extra='forbid')  # required for openai structured generation\n",
        "\n",
        "    query: str = Field(..., description=\"The generated question\")\n",
        "    answer: str = Field(..., description=\"The answer from the text\")\n",
        "    question_type: QuestionType = Field(..., description=\"Type of question\")\n",
        "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score between 0 and 1\")\n",
        "    requires_context: bool = Field(..., description=\"Whether additional context is needed\")\n",
        "    tokens_used: int = Field(..., ge=0, description=\"Number of tokens used\")\n",
        "\n",
        "@dataclass\n",
        "class ChunkResponse(BaseModel):\n",
        "    \"\"\"Response structure for query generation\"\"\"\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "    queries: List[QueryMetadata] = Field(..., description=\"List of generated queries and answers\")\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"Enhanced metadata for a document chunk including detailed Q&A pairs\"\"\"\n",
        "    chunk_id: int\n",
        "    text: str\n",
        "    queries: List[QueryMetadata]\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "    semantic_complexity: float = 0.0\n",
        "    key_entities: List[str] = Field(default_factory=list)\n",
        "\n",
        "class ChunkingStrategy(str, Enum):\n",
        "    RECURSIVE = \"recursive\"\n",
        "    CHARACTER = \"character\"\n",
        "    TOKEN = \"token\"\n",
        "    MARKDOWN = \"markdown\"\n",
        "\n",
        "\n",
        "class EntityResponse(BaseModel):\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "    entities: List[str] = Field(..., description=\"List of key entities extracted from the text\")\n",
        "\n",
        "async def _extract_key_entities(self, text: str) -> List[str]:\n",
        "    \"\"\"Extract key entities from chunk using direct JSON response\"\"\"\n",
        "    try:\n",
        "        print(\"Starting entity extraction...\")\n",
        "\n",
        "        prompt = f\"\"\"Extract key entities (important concepts, terms, or names) from this text.\n",
        "Return ONLY a JSON object with a single field 'entities' containing an array of strings.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Required format example:\n",
        "{{\"entities\": [\"entity1\", \"entity2\", \"entity3\"]}}\"\"\"\n",
        "\n",
        "        response = await self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "        print(f\"Extracted entities: {result['entities']}\")\n",
        "        return result['entities']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in entity extraction: {e}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "class AdvancedRAGGenerator:\n",
        "    def __init__(\n",
        "      self,\n",
        "      api_key: str,\n",
        "      embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
        "      chunk_size: int = 500,\n",
        "      chunk_overlap: int = 50,\n",
        "      queries_per_chunk: int = 3,\n",
        "      temperature: float = 0.7,\n",
        "      model: str = \"gpt-3.5-turbo\",\n",
        "      chunking_strategy: ChunkingStrategy = ChunkingStrategy.RECURSIVE,\n",
        "      question_types: List[QuestionType] = None,\n",
        "      min_confidence_threshold: float = 0.7,\n",
        "      separators: List[str] = None,\n",
        "      base_url: Optional[str] = None,\n",
        "      http_client: Optional[httpx.AsyncClient] = None\n",
        "  ):\n",
        "        \"\"\"\n",
        "        Initialize the advanced RAG ground truth generator\n",
        "\n",
        "        Args:\n",
        "            openai_client: OpenAI API compatible client\n",
        "            embedding_model: Model to use for semantic similarity\n",
        "            chunk_size: Size of each document chunk in characters/tokens\n",
        "            chunk_overlap: Overlap between chunks in characters/tokens\n",
        "            queries_per_chunk: Number of Q&A pairs to generate per chunk\n",
        "            temperature: Temperature for query generation\n",
        "            model: Model to use for generation\n",
        "            chunking_strategy: Strategy for document chunking\n",
        "            question_types: List of question types to generate\n",
        "            min_confidence_threshold: Minimum confidence score for generated Q&A pairs\n",
        "            separators: Custom separators for recursive splitting\n",
        "        \"\"\"\n",
        "        print(\"Initializing AdvancedRAGGenerator...\")  # Debug log\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.client = AsyncOpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=base_url,\n",
        "            http_client=http_client\n",
        "        )\n",
        "        print(f\"OpenAI client initialized with base_url: {base_url}\")  # Debug log\n",
        "\n",
        "        # Store model name\n",
        "        self.model = model\n",
        "        print(f\"Model name set to: {model}\")  # Debug log\n",
        "\n",
        "        # Configure and store outlines model\n",
        "        config = OpenAIConfig(\n",
        "            model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=2000  # Add reasonable max tokens\n",
        "        )\n",
        "        print(\"OpenAI config created\")  # Debug log\n",
        "\n",
        "        try:\n",
        "            self.openai_model = models.openai(self.client, config)\n",
        "            print(\"Outlines model successfully initialized\")  # Debug log\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing outlines model: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.queries_per_chunk = queries_per_chunk\n",
        "        self.temperature = temperature\n",
        "        self.chunking_strategy = chunking_strategy\n",
        "        self.question_types = question_types or list(QuestionType)\n",
        "        self.min_confidence_threshold = min_confidence_threshold\n",
        "        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "        self.embedding_model = AutoModel.from_pretrained(embedding_model)\n",
        "\n",
        "        # Initialize text splitter based on strategy\n",
        "        self._init_text_splitter()\n",
        "\n",
        "    def _init_text_splitter(self):\n",
        "        \"\"\"Initialize the appropriate text splitter based on strategy\"\"\"\n",
        "        if self.chunking_strategy == ChunkingStrategy.RECURSIVE:\n",
        "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap,\n",
        "                separators=self.separators\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.CHARACTER:\n",
        "            self.text_splitter = CharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap,\n",
        "                separator=\"\\n\"\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.TOKEN:\n",
        "            self.text_splitter = TokenTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.MARKDOWN:\n",
        "            headers_to_split_on = [\n",
        "                (\"#\", \"Header 1\"),\n",
        "                (\"##\", \"Header 2\"),\n",
        "                (\"###\", \"Header 3\"),\n",
        "            ]\n",
        "            self.text_splitter = MarkdownHeaderTextSplitter(\n",
        "                headers_to_split_on=headers_to_split_on\n",
        "            )\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for text using the embedding model\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.embedding_model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "    async def chunk_document(self, text: str) -> List[ChunkMetadata]:\n",
        "        \"\"\"Split document using LangChain text splitters\"\"\"\n",
        "        print(\"Starting chunk_document...\")  # Debug log\n",
        "        chunks = []\n",
        "\n",
        "        try:\n",
        "            if self.chunking_strategy == ChunkingStrategy.MARKDOWN:\n",
        "                raw_chunks = self.text_splitter.split_text(text)\n",
        "                start_char = 0\n",
        "\n",
        "                for i, chunk in enumerate(raw_chunks):\n",
        "                    print(f\"Processing chunk {i}\")  # Debug log\n",
        "                    chunk_text = chunk.page_content\n",
        "                    chunk_length = len(chunk_text)\n",
        "\n",
        "                    # Get entities with await\n",
        "                    entities = await self._extract_key_entities(chunk_text)\n",
        "                    print(f\"Got entities for chunk {i}: {entities}\")  # Debug log\n",
        "\n",
        "                    chunks.append(ChunkMetadata(\n",
        "                        chunk_id=i,\n",
        "                        text=chunk_text,\n",
        "                        queries=[],\n",
        "                        start_char=start_char,\n",
        "                        end_char=start_char + chunk_length,\n",
        "                        semantic_complexity=self._calculate_semantic_complexity(chunk_text),\n",
        "                        key_entities=entities\n",
        "                    ))\n",
        "                    start_char += chunk_length\n",
        "            else:\n",
        "                raw_chunks = self.text_splitter.split_text(text)\n",
        "                start_char = 0\n",
        "\n",
        "                for i, chunk_text in enumerate(raw_chunks):\n",
        "                    print(f\"Processing chunk {i}\")  # Debug log\n",
        "                    chunk_length = len(chunk_text)\n",
        "\n",
        "                    # Get entities with await\n",
        "                    entities = await self._extract_key_entities(chunk_text)\n",
        "                    print(f\"Got entities for chunk {i}: {entities}\")  # Debug log\n",
        "\n",
        "                    chunks.append(ChunkMetadata(\n",
        "                        chunk_id=i,\n",
        "                        text=chunk_text,\n",
        "                        queries=[],\n",
        "                        start_char=start_char,\n",
        "                        end_char=start_char + chunk_length,\n",
        "                        semantic_complexity=self._calculate_semantic_complexity(chunk_text),\n",
        "                        key_entities=entities\n",
        "                    ))\n",
        "                    start_char += chunk_length - self.chunk_overlap\n",
        "\n",
        "            print(f\"Completed chunk_document with {len(chunks)} chunks\")  # Debug log\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunk_document: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_semantic_complexity(self, text: str) -> float:\n",
        "        \"\"\"Calculate semantic complexity score for a chunk\"\"\"\n",
        "        embedding = self.get_embedding(text)\n",
        "        return float(np.var(embedding))\n",
        "\n",
        "    def _extract_key_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract key entities from chunk using the LLM\"\"\"\n",
        "        prompt = f\"\"\"Extract the key entities (important concepts, terms, or names) from the following text. Return as a JSON array of strings.\n",
        "\n",
        "Text: {text}\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "            return result.get(\"entities\", [])\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    async def generate_queries(self, chunk: ChunkMetadata) -> List[QueryMetadata]:\n",
        "        \"\"\"Generate diverse queries using direct JSON response\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"Create {self.queries_per_chunk} diverse questions and answers for this text.\n",
        "    Return ONLY a JSON object with a 'queries' array containing objects with these exact fields:\n",
        "    - query: the question text\n",
        "    - answer: the answer from the text\n",
        "    - question_type: one of [{', '.join(qt.value for qt in self.question_types)}]\n",
        "    - confidence_score: number between 0 and 1\n",
        "    - requires_context: boolean\n",
        "    - tokens_used: integer\n",
        "\n",
        "    Text to analyze:\n",
        "    {chunk.text}\n",
        "\n",
        "    Format example:\n",
        "    {{\n",
        "      \"queries\": [\n",
        "        {{\n",
        "          \"query\": \"What is required for wire transfers over $50,000?\",\n",
        "          \"answer\": \"Dual authorization from designated officers\",\n",
        "          \"question_type\": \"factual\",\n",
        "          \"confidence_score\": 0.95,\n",
        "          \"requires_context\": false,\n",
        "          \"tokens_used\": 42\n",
        "        }}\n",
        "      ]\n",
        "    }}\"\"\"\n",
        "\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "            # Convert the JSON response to QueryMetadata objects\n",
        "            queries = []\n",
        "            for q in result['queries']:\n",
        "                query = QueryMetadata(\n",
        "                    query=q['query'],\n",
        "                    answer=q['answer'],\n",
        "                    question_type=QuestionType(q['question_type']),\n",
        "                    confidence_score=float(q['confidence_score']),\n",
        "                    requires_context=bool(q['requires_context']),\n",
        "                    tokens_used=int(q['tokens_used'])\n",
        "                )\n",
        "                if query.confidence_score >= self.min_confidence_threshold:\n",
        "                    queries.append(query)\n",
        "\n",
        "            return queries\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating queries: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def evaluate_ground_truth(self, chunks: List[ChunkMetadata]) -> Dict:\n",
        "        \"\"\"Evaluate the quality of generated ground truth data\"\"\"\n",
        "        metrics = {\n",
        "            \"total_chunks\": len(chunks),\n",
        "            \"total_queries\": sum(len(chunk.queries) for chunk in chunks),\n",
        "            \"question_type_distribution\": defaultdict(int),\n",
        "            \"avg_confidence_score\": 0,\n",
        "            \"requires_context_percentage\": 0,\n",
        "            \"avg_semantic_complexity\": 0,\n",
        "            \"chunk_similarity_matrix\": None,\n",
        "            \"query_similarity_matrix\": None,\n",
        "            \"duplicate_queries\": 0,\n",
        "            \"coverage_score\": 0\n",
        "        }\n",
        "\n",
        "        # Calculate basic metrics\n",
        "        total_queries = 0\n",
        "        total_confidence = 0\n",
        "        requires_context = 0\n",
        "\n",
        "        for chunk in chunks:\n",
        "            metrics[\"avg_semantic_complexity\"] += chunk.semantic_complexity\n",
        "            for query in chunk.queries:\n",
        "                total_queries += 1\n",
        "                total_confidence += query.confidence_score\n",
        "                requires_context += int(query.requires_context)\n",
        "                metrics[\"question_type_distribution\"][query.question_type] += 1\n",
        "\n",
        "        if total_queries > 0:\n",
        "            metrics[\"avg_confidence_score\"] = total_confidence / total_queries\n",
        "            metrics[\"requires_context_percentage\"] = (requires_context / total_queries * 100)\n",
        "            metrics[\"avg_semantic_complexity\"] /= len(chunks)\n",
        "\n",
        "        # Calculate chunk similarity matrix\n",
        "        chunk_embeddings = np.vstack([self.get_embedding(chunk.text) for chunk in chunks])\n",
        "        metrics[\"chunk_similarity_matrix\"] = cosine_similarity(chunk_embeddings)\n",
        "\n",
        "        # Calculate query similarity and detect duplicates\n",
        "        all_queries = []\n",
        "        for chunk in chunks:\n",
        "            all_queries.extend([q.query for q in chunk.queries])\n",
        "\n",
        "        if all_queries:  # Only calculate if we have queries\n",
        "            query_embeddings = np.vstack([self.get_embedding(q) for q in all_queries])\n",
        "            query_similarities = cosine_similarity(query_embeddings)\n",
        "\n",
        "            # Count highly similar queries (potential duplicates)\n",
        "            duplicate_threshold = 0.95\n",
        "            metrics[\"duplicate_queries\"] = sum(sum(row > duplicate_threshold) for row in query_similarities) - len(all_queries)\n",
        "\n",
        "            # Calculate coverage score\n",
        "            coverage_scores = []\n",
        "            for chunk in chunks:\n",
        "                chunk_embedding = self.get_embedding(chunk.text)\n",
        "                query_embeddings = np.vstack([self.get_embedding(q.query) for q in chunk.queries])\n",
        "                max_similarities = np.max(cosine_similarity(query_embeddings, chunk_embedding), axis=0)\n",
        "                coverage_scores.append(np.mean(max_similarities))\n",
        "\n",
        "            metrics[\"coverage_score\"] = np.mean(coverage_scores)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    async def test_entity_extraction():\n",
        "        \"\"\"Test function for entity extraction\"\"\"\n",
        "        test_text = \"This is a test text about banking and transactions.\"\n",
        "        try:\n",
        "            print(\"Testing entity extraction...\")\n",
        "            entities = await generator._extract_key_entities(test_text)\n",
        "            print(f\"Successfully extracted entities: {entities}\")\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            print(f\"Error in test: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def test_simple_completion(self):\n",
        "        \"\"\"Test basic outlines completion\"\"\"\n",
        "        try:\n",
        "            print(\"Testing simple completion...\")\n",
        "\n",
        "            class SimpleResponse(BaseModel):\n",
        "                model_config = ConfigDict(extra='forbid')\n",
        "                text: str = Field(..., description=\"Simple response text\")\n",
        "\n",
        "            generator = generate.json(self.openai_model, SimpleResponse)\n",
        "            response = await generator(\"Say hello\")\n",
        "            print(f\"Response: {response}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Simple test failed: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def process_document(self, document: str) -> List[ChunkMetadata]:\n",
        "        \"\"\"Process document and generate evaluated ground truth data\"\"\"\n",
        "        print(\"Starting document processing...\")\n",
        "        print(f\"Using model: {self.model}\")\n",
        "        print(f\"Client initialized: {self.client is not None}\")\n",
        "        print(f\"Outlines model initialized: {self.openai_model is not None}\")\n",
        "\n",
        "        try:\n",
        "            chunks = await self.chunk_document(document)\n",
        "            print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "            processed_chunks = []\n",
        "            for chunk in tqdm(chunks, desc=\"Generating queries\"):\n",
        "                try:\n",
        "                    queries = await self.generate_queries(chunk)\n",
        "                    chunk.queries = queries\n",
        "                    processed_chunks.append(chunk)\n",
        "                    print(f\"Generated {len(queries)} queries for chunk {chunk.chunk_id}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chunk {chunk.chunk_id}: {e}\")\n",
        "                    print(f\"Full exception: {repr(e)}\")\n",
        "\n",
        "            return processed_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_document: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            raise\n",
        "\n",
        "    def save_ground_truth(self, chunks: List[ChunkMetadata], output_file: str):\n",
        "        \"\"\"Save ground truth data with evaluation metrics\"\"\"\n",
        "        evaluation = self.evaluate_ground_truth(chunks)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        if \"chunk_similarity_matrix\" in evaluation:\n",
        "            evaluation[\"chunk_similarity_matrix\"] = evaluation[\"chunk_similarity_matrix\"].tolist()\n",
        "\n",
        "        data = {\n",
        "            \"chunks\": [asdict(chunk) for chunk in chunks],\n",
        "            \"evaluation_metrics\": evaluation\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('API_KEY')\n",
        "base_url = userdata.get('LLAMA_BASE_URL')\n",
        "\n",
        "import httpx\n",
        "import asyncio\n",
        "\n",
        "# Optional custom HTTP client\n",
        "# http_client = httpx.AsyncClient(\n",
        "#     proxies=\"http://my.proxy.example.com\",\n",
        "#     transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n",
        "# )\n",
        "\n",
        "# Initialize generator with debug mode\n",
        "generator = AdvancedRAGGenerator(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url,\n",
        "    # http_client=http_client,  # optional\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    queries_per_chunk=3,\n",
        "    model=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\"  # or your specific model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJlCEo6AhsqY",
        "outputId": "7ac546b3-23e7-4996-c47e-c669ca882b08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedRAGGenerator...\n",
            "OpenAI client initialized with base_url: https://wesslen--vllm-openai-compatible-serve.modal.run/v1\n",
            "Model name set to: /models/NousResearch/Meta-Llama-3.1-8B-Instruct\n",
            "OpenAI config created\n",
            "Outlines model successfully initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document (banking policies)\n",
        "policy_document = '''\n",
        "# Internal Banking Policy Manual\n",
        "\n",
        "## Section 1: Transaction Processing\n",
        "\n",
        "### 1.1 Wire Transfer Authorization\n",
        "All wire transfers exceeding $50,000 require dual authorization from designated officers.\n",
        "The primary authorizer must be a department manager or above, while the secondary\n",
        "authorizer must be from a different department to ensure segregation of duties.\n",
        "\n",
        "### 1.2 Customer Authentication\n",
        "Customer identity must be verified through two-factor authentication for all high-risk\n",
        "transactions. This includes:\n",
        "- Wire transfers above $10,000\n",
        "- Changes to account ownership\n",
        "- Updates to primary contact information\n",
        "\n",
        "## Section 2: Suspicious Activity Reporting\n",
        "\n",
        "### 2.1 Reporting Requirements\n",
        "Staff must report any suspicious transactions through the SAR filing system within 24\n",
        "hours of detection. Red flags include:\n",
        "- Structured deposits just below reporting thresholds\n",
        "- Rapid movement of funds between accounts\n",
        "- Multiple high-value transactions from dormant accounts\n",
        "'''\n"
      ],
      "metadata": {
        "id": "tzkCp0G6VXu8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate example"
      ],
      "metadata": {
        "id": "4S2gSirzVSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# minimum test\n",
        "async def test_minimal():\n",
        "    try:\n",
        "        # Initialize with minimal configuration\n",
        "        gen = AdvancedRAGGenerator(\n",
        "            api_key=api_key,\n",
        "            base_url=base_url,\n",
        "            model=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
        "        )\n",
        "\n",
        "        # Create chat completion with JSON response format\n",
        "        response = await gen.client.chat.completions.create(\n",
        "            model=gen.model,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Return a simple JSON response with a 'text' field containing 'hello'. Format: {\\\"text\\\": \\\"hello\\\"}\"\n",
        "            }],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        print(f\"Test result: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Minimal test failed: {e}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run minimal test\n",
        "await test_minimal()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2RZXbmZrKeZ",
        "outputId": "20bdcafb-13b2-40b2-9fea-81d803382bc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedRAGGenerator...\n",
            "OpenAI client initialized with base_url: https://wesslen--vllm-openai-compatible-serve.modal.run/v1\n",
            "Model name set to: /models/NousResearch/Meta-Llama-3.1-8B-Instruct\n",
            "OpenAI config created\n",
            "Outlines model successfully initialized\n",
            "Test result: ChatCompletion(id='chat-800d1df65f624f7d8ee58bebb95f22ca', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \\n \"text\" \\t: \\t\"hello\" \\n}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1736051842, model='/models/NousResearch/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=33, total_tokens=49, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified main function\n",
        "async def main():\n",
        "    try:\n",
        "        print(\"Starting processing...\")\n",
        "        print(f\"Generator initialized with model: {generator.model}\")\n",
        "\n",
        "        # Run simple test first\n",
        "        print(\"Running simple completion test...\")\n",
        "        if not await generator.test_simple_completion():  # Note the generator. prefix\n",
        "            print(\"Simple test failed, stopping processing\")\n",
        "            return None\n",
        "\n",
        "        # Then test entity extraction\n",
        "        print(\"Running entity extraction test...\")\n",
        "        test_result = await generator.test_entity_extraction()\n",
        "        if test_result is None:\n",
        "            print(\"Entity extraction test failed, stopping processing\")\n",
        "            return None\n",
        "\n",
        "        print(\"Tests successful, proceeding with document processing\")\n",
        "        chunks = await generator.process_document(policy_document)\n",
        "        print(\"Processing complete!\")\n",
        "        generator.save_ground_truth(chunks, 'ground_truth.json')\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "cJ0xQUhVh2k0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: debug chunk above. When run, nothing is output. Need to debug."
      ],
      "metadata": {
        "id": "NfyVNAV9wS52"
      }
    }
  ]
}