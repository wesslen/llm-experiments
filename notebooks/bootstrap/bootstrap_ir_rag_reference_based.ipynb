{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyCNB8ez6zuqH/RF6z+GFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesslen/llm-experiments/blob/main/notebooks/bootstrap/bootstrap_ir_rag_reference_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install --system \"outlines[openai]\""
      ],
      "metadata": {
        "id": "cnfSHRtph5nX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EbZKAbxiVOyY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, List, Optional, Union, Literal\n",
        "from dataclasses import dataclass, asdict\n",
        "from pydantic import BaseModel, ConfigDict, Field\n",
        "from outlines.models.openai import OpenAIConfig\n",
        "from outlines import models, generate\n",
        "import openai\n",
        "from openai import AsyncOpenAI, DefaultHttpxClient\n",
        "import httpx\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from enum import Enum\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from collections import defaultdict\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        ")\n",
        "\n",
        "class QuestionType(str, Enum):\n",
        "    FACTUAL = \"factual\"\n",
        "    PROCEDURAL = \"procedural\"\n",
        "    CONCEPTUAL = \"conceptual\"\n",
        "    COMPARATIVE = \"comparative\"\n",
        "    ANALYTICAL = \"analytical\"\n",
        "\n",
        "@dataclass\n",
        "class QueryMetadata(BaseModel):\n",
        "    \"\"\"Metadata for a generated query\"\"\"\n",
        "    model_config = ConfigDict(extra='forbid')  # required for openai structured generation\n",
        "\n",
        "    query: str = Field(..., description=\"The generated question\")\n",
        "    answer: str = Field(..., description=\"The answer from the text\")\n",
        "    question_type: QuestionType = Field(..., description=\"Type of question\")\n",
        "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence score between 0 and 1\")\n",
        "    requires_context: bool = Field(..., description=\"Whether additional context is needed\")\n",
        "    tokens_used: int = Field(..., ge=0, description=\"Number of tokens used\")\n",
        "\n",
        "@dataclass\n",
        "class ChunkResponse(BaseModel):\n",
        "    \"\"\"Response structure for query generation\"\"\"\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "    queries: List[QueryMetadata] = Field(..., description=\"List of generated queries and answers\")\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"Enhanced metadata for a document chunk including detailed Q&A pairs\"\"\"\n",
        "    chunk_id: int\n",
        "    text: str\n",
        "    queries: List[QueryMetadata]\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "    semantic_complexity: float = 0.0\n",
        "    key_entities: List[str] = Field(default_factory=list)\n",
        "\n",
        "class ChunkingStrategy(str, Enum):\n",
        "    RECURSIVE = \"recursive\"\n",
        "    CHARACTER = \"character\"\n",
        "    TOKEN = \"token\"\n",
        "    MARKDOWN = \"markdown\"\n",
        "\n",
        "\n",
        "class EntityResponse(BaseModel):\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "    entities: List[str] = Field(..., description=\"List of key entities extracted from the text\")\n",
        "\n",
        "async def _extract_key_entities(self, text: str) -> List[str]:\n",
        "    \"\"\"Extract key entities from chunk using direct JSON response\"\"\"\n",
        "    try:\n",
        "        print(\"Starting entity extraction...\")\n",
        "\n",
        "        prompt = f\"\"\"Extract key entities (important concepts, terms, or names) from this text.\n",
        "Return ONLY a JSON object with a single field 'entities' containing an array of strings.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Required format example:\n",
        "{{\"entities\": [\"entity1\", \"entity2\", \"entity3\"]}}\"\"\"\n",
        "\n",
        "        response = await self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "        print(f\"Extracted entities: {result['entities']}\")\n",
        "        return result['entities']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in entity extraction: {e}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "class AdvancedRAGGenerator:\n",
        "    def __init__(\n",
        "      self,\n",
        "      api_key: str,\n",
        "      embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
        "      chunk_size: int = 500,\n",
        "      chunk_overlap: int = 50,\n",
        "      queries_per_chunk: int = 3,\n",
        "      temperature: float = 0.7,\n",
        "      model: str = \"gpt-3.5-turbo\",\n",
        "      chunking_strategy: ChunkingStrategy = ChunkingStrategy.RECURSIVE,\n",
        "      question_types: List[QuestionType] = None,\n",
        "      min_confidence_threshold: float = 0.7,\n",
        "      separators: List[str] = None,\n",
        "      base_url: Optional[str] = None,\n",
        "      http_client: Optional[httpx.AsyncClient] = None\n",
        "  ):\n",
        "        \"\"\"\n",
        "        Initialize the advanced RAG ground truth generator\n",
        "\n",
        "        Args:\n",
        "            openai_client: OpenAI API compatible client\n",
        "            embedding_model: Model to use for semantic similarity\n",
        "            chunk_size: Size of each document chunk in characters/tokens\n",
        "            chunk_overlap: Overlap between chunks in characters/tokens\n",
        "            queries_per_chunk: Number of Q&A pairs to generate per chunk\n",
        "            temperature: Temperature for query generation\n",
        "            model: Model to use for generation\n",
        "            chunking_strategy: Strategy for document chunking\n",
        "            question_types: List of question types to generate\n",
        "            min_confidence_threshold: Minimum confidence score for generated Q&A pairs\n",
        "            separators: Custom separators for recursive splitting\n",
        "        \"\"\"\n",
        "        print(\"Initializing AdvancedRAGGenerator...\")  # Debug log\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.client = AsyncOpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=base_url,\n",
        "            http_client=http_client\n",
        "        )\n",
        "        print(f\"OpenAI client initialized with base_url: {base_url}\")  # Debug log\n",
        "\n",
        "        # Store model name\n",
        "        self.model = model\n",
        "        print(f\"Model name set to: {model}\")  # Debug log\n",
        "\n",
        "        # Configure and store outlines model\n",
        "        config = OpenAIConfig(\n",
        "            model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=2000  # Add reasonable max tokens\n",
        "        )\n",
        "        print(\"OpenAI config created\")  # Debug log\n",
        "\n",
        "        try:\n",
        "            self.openai_model = models.openai(self.client, config)\n",
        "            print(\"Outlines model successfully initialized\")  # Debug log\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing outlines model: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.queries_per_chunk = queries_per_chunk\n",
        "        self.temperature = temperature\n",
        "        self.chunking_strategy = chunking_strategy\n",
        "        self.question_types = question_types or list(QuestionType)\n",
        "        self.min_confidence_threshold = min_confidence_threshold\n",
        "        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "        self.embedding_model = AutoModel.from_pretrained(embedding_model)\n",
        "\n",
        "        # Initialize text splitter based on strategy\n",
        "        self._init_text_splitter()\n",
        "\n",
        "    def _init_text_splitter(self):\n",
        "        \"\"\"Initialize the appropriate text splitter based on strategy\"\"\"\n",
        "        if self.chunking_strategy == ChunkingStrategy.RECURSIVE:\n",
        "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap,\n",
        "                separators=self.separators\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.CHARACTER:\n",
        "            self.text_splitter = CharacterTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap,\n",
        "                separator=\"\\n\"\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.TOKEN:\n",
        "            self.text_splitter = TokenTextSplitter(\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap\n",
        "            )\n",
        "        elif self.chunking_strategy == ChunkingStrategy.MARKDOWN:\n",
        "            headers_to_split_on = [\n",
        "                (\"#\", \"Header 1\"),\n",
        "                (\"##\", \"Header 2\"),\n",
        "                (\"###\", \"Header 3\"),\n",
        "            ]\n",
        "            self.text_splitter = MarkdownHeaderTextSplitter(\n",
        "                headers_to_split_on=headers_to_split_on\n",
        "            )\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for text using the embedding model\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.embedding_model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "    async def chunk_document(self, text: str) -> List[ChunkMetadata]:\n",
        "        \"\"\"Split document using LangChain text splitters\"\"\"\n",
        "        print(\"Starting chunk_document...\")  # Debug log\n",
        "        chunks = []\n",
        "\n",
        "        try:\n",
        "            if self.chunking_strategy == ChunkingStrategy.MARKDOWN:\n",
        "                raw_chunks = self.text_splitter.split_text(text)\n",
        "                start_char = 0\n",
        "\n",
        "                for i, chunk in enumerate(raw_chunks):\n",
        "                    print(f\"Processing chunk {i}\")  # Debug log\n",
        "                    chunk_text = chunk.page_content\n",
        "                    chunk_length = len(chunk_text)\n",
        "\n",
        "                    # Get entities with await\n",
        "                    entities = await self._extract_key_entities(chunk_text)\n",
        "                    print(f\"Got entities for chunk {i}: {entities}\")  # Debug log\n",
        "\n",
        "                    chunks.append(ChunkMetadata(\n",
        "                        chunk_id=i,\n",
        "                        text=chunk_text,\n",
        "                        queries=[],\n",
        "                        start_char=start_char,\n",
        "                        end_char=start_char + chunk_length,\n",
        "                        semantic_complexity=self._calculate_semantic_complexity(chunk_text),\n",
        "                        key_entities=entities\n",
        "                    ))\n",
        "                    start_char += chunk_length\n",
        "            else:\n",
        "                raw_chunks = self.text_splitter.split_text(text)\n",
        "                start_char = 0\n",
        "\n",
        "                for i, chunk_text in enumerate(raw_chunks):\n",
        "                    print(f\"Processing chunk {i}\")  # Debug log\n",
        "                    chunk_length = len(chunk_text)\n",
        "\n",
        "                    # Get entities with await\n",
        "                    entities = await self._extract_key_entities(chunk_text)\n",
        "                    print(f\"Got entities for chunk {i}: {entities}\")  # Debug log\n",
        "\n",
        "                    chunks.append(ChunkMetadata(\n",
        "                        chunk_id=i,\n",
        "                        text=chunk_text,\n",
        "                        queries=[],\n",
        "                        start_char=start_char,\n",
        "                        end_char=start_char + chunk_length,\n",
        "                        semantic_complexity=self._calculate_semantic_complexity(chunk_text),\n",
        "                        key_entities=entities\n",
        "                    ))\n",
        "                    start_char += chunk_length - self.chunk_overlap\n",
        "\n",
        "            print(f\"Completed chunk_document with {len(chunks)} chunks\")  # Debug log\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunk_document: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_semantic_complexity(self, text: str) -> float:\n",
        "        \"\"\"Calculate semantic complexity score for a chunk\"\"\"\n",
        "        embedding = self.get_embedding(text)\n",
        "        return float(np.var(embedding))\n",
        "\n",
        "    # Update test_entity_extraction to match\n",
        "    async def test_entity_extraction(self):\n",
        "        \"\"\"Test function for entity extraction\"\"\"\n",
        "        test_text = \"This is a test text about banking and transactions.\"\n",
        "        try:\n",
        "            print(\"Testing entity extraction...\")\n",
        "            entities = await self._extract_key_entities(test_text)\n",
        "            print(f\"Successfully extracted entities: {entities}\")\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            print(f\"Error in test: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return None\n",
        "\n",
        "    async def test_simple_completion(self):\n",
        "        \"\"\"Test basic completion with simple JSON response\"\"\"\n",
        "        try:\n",
        "            print(\"Testing simple completion...\")\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Return only a JSON object with format: {\\\"text\\\": \\\"hello world\\\"}\"\n",
        "                }],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "\n",
        "            print(f\"Response received: {response.choices[0].message.content}\")\n",
        "            # Try parsing the JSON to verify it's valid\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "            if \"text\" in result:\n",
        "                print(f\"Successfully parsed response: {result}\")\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Simple test failed: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def _extract_key_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract key entities from chunk using direct JSON response\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"You are a helpful assistant that extracts key entities from text.\n",
        "\n",
        "    Your task: Read the text below and identify the main entities (important terms, concepts, numbers, or names).\n",
        "\n",
        "    Text to analyze: {text}\n",
        "\n",
        "    Instructions:\n",
        "    1. Extract all important entities (nouns, terms, numbers, names)\n",
        "    2. Return them in a JSON array\n",
        "    3. Use EXACTLY this format: {{\"entities\": [\"entity1\", \"entity2\", \"entity3\"]}}\n",
        "\n",
        "    Example output format:\n",
        "    {{\"entities\": [\"wire transfer\", \"$50,000\", \"dual authorization\", \"officers\"]}}\"\"\"\n",
        "\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a precise entity extraction assistant that responds only in JSON format.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "\n",
        "            print(f\"Entity extraction raw response: {response.choices[0].message.content}\")\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "            # Check if we got an error response\n",
        "            if \"errors\" in result:\n",
        "                print(f\"Got error response: {result['errors']}\")\n",
        "                # Try to extract entities from error message if possible\n",
        "                return []\n",
        "\n",
        "            entities = result.get('entities', [])\n",
        "            if not entities:\n",
        "                print(\"Warning: No entities extracted\")\n",
        "            else:\n",
        "                print(f\"Successfully extracted {len(entities)} entities\")\n",
        "\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in entity extraction: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return []\n",
        "\n",
        "    async def generate_queries(self, chunk: ChunkMetadata) -> List[QueryMetadata]:\n",
        "        \"\"\"Generate diverse queries using direct JSON response\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"You are a helpful assistant that generates questions and answers from text.\n",
        "\n",
        "    Text to analyze:\n",
        "    {chunk.text}\n",
        "\n",
        "    Task: Create {self.queries_per_chunk} diverse questions and answers based on this text.\n",
        "\n",
        "    Instructions:\n",
        "    1. Generate questions that test different aspects of the content\n",
        "    2. Provide accurate answers from the text\n",
        "    3. Return the response in the exact JSON format shown below\n",
        "\n",
        "    Required JSON format:\n",
        "    {{\n",
        "        \"queries\": [\n",
        "            {{\n",
        "                \"query\": \"Your question here?\",\n",
        "                \"answer\": \"Answer from the text\",\n",
        "                \"question_type\": \"factual\",\n",
        "                \"confidence_score\": 0.95,\n",
        "                \"requires_context\": false,\n",
        "                \"tokens_used\": 42\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Notes:\n",
        "    - question_type must be one of: factual, procedural, conceptual, comparative, analytical\n",
        "    - confidence_score must be between 0 and 1\n",
        "    - requires_context should be true if the answer needs more context\n",
        "    - tokens_used can be estimated\"\"\"\n",
        "\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a precise question generation assistant that responds only in JSON format.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "\n",
        "            print(f\"Query generation raw response: {response.choices[0].message.content}\")\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "            # Check if we got an error response\n",
        "            if \"errors\" in result:\n",
        "                print(f\"Got error response: {result['errors']}\")\n",
        "                return []\n",
        "\n",
        "            # Convert the JSON response to QueryMetadata objects\n",
        "            queries = []\n",
        "            for q in result.get('queries', []):\n",
        "                try:\n",
        "                    query = QueryMetadata(\n",
        "                        query=q['query'],\n",
        "                        answer=q['answer'],\n",
        "                        question_type=QuestionType(q['question_type']),\n",
        "                        confidence_score=float(q['confidence_score']),\n",
        "                        requires_context=bool(q['requires_context']),\n",
        "                        tokens_used=int(q['tokens_used'])\n",
        "                    )\n",
        "                    if query.confidence_score >= self.min_confidence_threshold:\n",
        "                        queries.append(query)\n",
        "                        print(f\"Added query: {query.query}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing query: {e}\")\n",
        "                    continue\n",
        "\n",
        "            return queries\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating queries: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def evaluate_ground_truth(self, chunks: List[ChunkMetadata]) -> Dict:\n",
        "        \"\"\"Evaluate the quality of generated ground truth data\"\"\"\n",
        "        metrics = {\n",
        "            \"total_chunks\": len(chunks),\n",
        "            \"total_queries\": sum(len(chunk.queries) for chunk in chunks),\n",
        "            \"question_type_distribution\": defaultdict(int),\n",
        "            \"avg_confidence_score\": 0,\n",
        "            \"requires_context_percentage\": 0,\n",
        "            \"avg_semantic_complexity\": 0,\n",
        "            \"chunk_similarity_matrix\": None,\n",
        "            \"query_similarity_matrix\": None,\n",
        "            \"duplicate_queries\": 0,\n",
        "            \"coverage_score\": 0\n",
        "        }\n",
        "\n",
        "        # Calculate basic metrics\n",
        "        total_queries = 0\n",
        "        total_confidence = 0\n",
        "        requires_context = 0\n",
        "\n",
        "        for chunk in chunks:\n",
        "            metrics[\"avg_semantic_complexity\"] += chunk.semantic_complexity\n",
        "            for query in chunk.queries:\n",
        "                total_queries += 1\n",
        "                total_confidence += query.confidence_score\n",
        "                requires_context += int(query.requires_context)\n",
        "                metrics[\"question_type_distribution\"][query.question_type] += 1\n",
        "\n",
        "        if total_queries > 0:\n",
        "            metrics[\"avg_confidence_score\"] = total_confidence / total_queries\n",
        "            metrics[\"requires_context_percentage\"] = (requires_context / total_queries * 100)\n",
        "            metrics[\"avg_semantic_complexity\"] /= len(chunks)\n",
        "\n",
        "        # Calculate chunk similarity matrix\n",
        "        chunk_embeddings = np.vstack([self.get_embedding(chunk.text) for chunk in chunks])\n",
        "        metrics[\"chunk_similarity_matrix\"] = cosine_similarity(chunk_embeddings)\n",
        "\n",
        "        # Calculate query similarity and detect duplicates\n",
        "        all_queries = []\n",
        "        for chunk in chunks:\n",
        "            all_queries.extend([q.query for q in chunk.queries])\n",
        "\n",
        "        if all_queries:  # Only calculate if we have queries\n",
        "            query_embeddings = np.vstack([self.get_embedding(q) for q in all_queries])\n",
        "            query_similarities = cosine_similarity(query_embeddings)\n",
        "\n",
        "            # Count highly similar queries (potential duplicates)\n",
        "            duplicate_threshold = 0.95\n",
        "            metrics[\"duplicate_queries\"] = sum(sum(row > duplicate_threshold) for row in query_similarities) - len(all_queries)\n",
        "\n",
        "            # Calculate coverage score\n",
        "            coverage_scores = []\n",
        "            for chunk in chunks:\n",
        "                chunk_embedding = self.get_embedding(chunk.text)\n",
        "                query_embeddings = np.vstack([self.get_embedding(q.query) for q in chunk.queries])\n",
        "                max_similarities = np.max(cosine_similarity(query_embeddings, chunk_embedding), axis=0)\n",
        "                coverage_scores.append(np.mean(max_similarities))\n",
        "\n",
        "            metrics[\"coverage_score\"] = np.mean(coverage_scores)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    async def process_document(self, document: str) -> List[ChunkMetadata]:\n",
        "        \"\"\"Process document and generate evaluated ground truth data\"\"\"\n",
        "        print(\"\\n=== Starting document processing ===\")\n",
        "        print(f\"Document length: {len(document)} characters\")\n",
        "        print(f\"Using model: {self.model}\")\n",
        "        print(f\"Client initialized: {self.client is not None}\")\n",
        "        print(f\"Outlines model initialized: {self.openai_model is not None}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Chunking document ---\")\n",
        "            chunks = await self.chunk_document(document)\n",
        "            print(f\"Created {len(chunks)} chunks\")\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                print(f\"\\nChunk {i} preview:\")\n",
        "                print(f\"- Length: {len(chunk.text)} characters\")\n",
        "                print(f\"- First 100 chars: {chunk.text[:100]}...\")\n",
        "                print(f\"- Entities: {chunk.key_entities}\")\n",
        "\n",
        "            processed_chunks = []\n",
        "            for chunk in tqdm(chunks, desc=\"Generating queries\"):\n",
        "                try:\n",
        "                    print(f\"\\n--- Processing chunk {chunk.chunk_id} ---\")\n",
        "                    queries = await self.generate_queries(chunk)\n",
        "                    print(f\"Generated queries: {len(queries)}\")\n",
        "                    for q in queries:\n",
        "                        print(f\"- Q: {q.query}\")\n",
        "                        print(f\"  A: {q.answer}\")\n",
        "                    chunk.queries = queries\n",
        "                    processed_chunks.append(chunk)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chunk {chunk.chunk_id}: {e}\")\n",
        "                    print(f\"Full exception: {repr(e)}\")\n",
        "\n",
        "            return processed_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_document: {e}\")\n",
        "            print(f\"Full exception: {repr(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def save_ground_truth(self, chunks: List[ChunkMetadata], output_file: str):\n",
        "        \"\"\"Save ground truth data with evaluation metrics\"\"\"\n",
        "        evaluation = self.evaluate_ground_truth(chunks)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        if \"chunk_similarity_matrix\" in evaluation:\n",
        "            evaluation[\"chunk_similarity_matrix\"] = evaluation[\"chunk_similarity_matrix\"].tolist()\n",
        "\n",
        "        data = {\n",
        "            \"chunks\": [asdict(chunk) for chunk in chunks],\n",
        "            \"evaluation_metrics\": evaluation\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('API_KEY')\n",
        "base_url = userdata.get('LLAMA_BASE_URL')\n",
        "\n",
        "import httpx\n",
        "import asyncio\n",
        "\n",
        "# Optional custom HTTP client\n",
        "# http_client = httpx.AsyncClient(\n",
        "#     proxies=\"http://my.proxy.example.com\",\n",
        "#     transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n",
        "# )\n",
        "\n",
        "# Initialize generator with debug mode\n",
        "generator = AdvancedRAGGenerator(\n",
        "    api_key=api_key,\n",
        "    base_url=base_url,\n",
        "    # http_client=http_client,  # optional\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    queries_per_chunk=3,\n",
        "    model=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\"  # or your specific model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJlCEo6AhsqY",
        "outputId": "6760ad8c-7115-4c81-e1ec-cfe431aba004"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedRAGGenerator...\n",
            "OpenAI client initialized with base_url: https://wesslen--vllm-openai-compatible-serve.modal.run/v1\n",
            "Model name set to: /models/NousResearch/Meta-Llama-3.1-8B-Instruct\n",
            "OpenAI config created\n",
            "Outlines model successfully initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document (banking policies)\n",
        "policy_document = '''\n",
        "# Internal Banking Policy Manual\n",
        "\n",
        "## Section 1: Transaction Processing\n",
        "\n",
        "### 1.1 Wire Transfer Authorization\n",
        "All wire transfers exceeding $50,000 require dual authorization from designated officers.\n",
        "The primary authorizer must be a department manager or above, while the secondary\n",
        "authorizer must be from a different department to ensure segregation of duties.\n",
        "\n",
        "### 1.2 Customer Authentication\n",
        "Customer identity must be verified through two-factor authentication for all high-risk\n",
        "transactions. This includes:\n",
        "- Wire transfers above $10,000\n",
        "- Changes to account ownership\n",
        "- Updates to primary contact information\n",
        "\n",
        "## Section 2: Suspicious Activity Reporting\n",
        "\n",
        "### 2.1 Reporting Requirements\n",
        "Staff must report any suspicious transactions through the SAR filing system within 24\n",
        "hours of detection. Red flags include:\n",
        "- Structured deposits just below reporting thresholds\n",
        "- Rapid movement of funds between accounts\n",
        "- Multiple high-value transactions from dormant accounts\n",
        "'''\n"
      ],
      "metadata": {
        "id": "tzkCp0G6VXu8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate example"
      ],
      "metadata": {
        "id": "4S2gSirzVSIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# minimum test\n",
        "async def test_minimal():\n",
        "    try:\n",
        "        # Initialize with minimal configuration\n",
        "        gen = AdvancedRAGGenerator(\n",
        "            api_key=api_key,\n",
        "            base_url=base_url,\n",
        "            model=\"/models/NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
        "        )\n",
        "\n",
        "        # Create chat completion with JSON response format\n",
        "        response = await gen.client.chat.completions.create(\n",
        "            model=gen.model,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Return a simple JSON response with a 'text' field containing 'hello'. Format: {\\\"text\\\": \\\"hello\\\"}\"\n",
        "            }],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        print(f\"Test result: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Minimal test failed: {e}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run minimal test\n",
        "await test_minimal()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2RZXbmZrKeZ",
        "outputId": "49f6b9b9-8ef6-4bce-dd65-f705712b322a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AdvancedRAGGenerator...\n",
            "OpenAI client initialized with base_url: https://wesslen--vllm-openai-compatible-serve.modal.run/v1\n",
            "Model name set to: /models/NousResearch/Meta-Llama-3.1-8B-Instruct\n",
            "OpenAI config created\n",
            "Outlines model successfully initialized\n",
            "Test result: ChatCompletion(id='chat-849b3e7fced04627893e23e3ed59b8c6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \\n \"text\" \\t: \\t\"hello\" \\n}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1736052780, model='/models/NousResearch/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=33, total_tokens=49, completion_tokens_details=None, prompt_tokens_details=None))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing the changes\n",
        "async def test_run():\n",
        "    print(\"\\n=== Testing Individual Components ===\")\n",
        "\n",
        "    test_text = \"\"\"Wire transfers exceeding $50,000 require dual authorization from designated officers.\n",
        "The primary authorizer must be a department manager or above.\"\"\"\n",
        "\n",
        "    # Test 1: Simple completion\n",
        "    print(\"\\n--- Test 1: Simple Completion ---\")\n",
        "    simple_result = await generator.test_simple_completion()\n",
        "    print(f\"Simple completion test result: {simple_result}\")\n",
        "\n",
        "    if not simple_result:\n",
        "        print(\"Simple completion test failed, stopping tests\")\n",
        "        return\n",
        "\n",
        "    # Test 2: Entity extraction\n",
        "    print(\"\\n--- Test 2: Entity Extraction ---\")\n",
        "    print(f\"Testing with text:\\n{test_text}\")\n",
        "    entities = await generator._extract_key_entities(test_text)\n",
        "    print(f\"Extracted entities: {entities}\")\n",
        "\n",
        "    if not entities:\n",
        "        print(\"Entity extraction returned no results, but continuing...\")\n",
        "\n",
        "    # Test 3: Generate queries for a small chunk\n",
        "    print(\"\\n--- Test 3: Query Generation ---\")\n",
        "    test_chunk = ChunkMetadata(\n",
        "        chunk_id=0,\n",
        "        text=test_text,\n",
        "        queries=[],\n",
        "        start_char=0,\n",
        "        end_char=len(test_text),\n",
        "        key_entities=entities\n",
        "    )\n",
        "\n",
        "    print(\"Generating queries for test chunk...\")\n",
        "    queries = await generator.generate_queries(test_chunk)\n",
        "\n",
        "    if queries:\n",
        "        print(\"\\nGenerated Queries:\")\n",
        "        for i, q in enumerate(queries, 1):\n",
        "            print(f\"\\nQuery {i}:\")\n",
        "            print(f\"Q: {q.query}\")\n",
        "            print(f\"A: {q.answer}\")\n",
        "            print(f\"Type: {q.question_type}\")\n",
        "            print(f\"Confidence: {q.confidence_score}\")\n",
        "    else:\n",
        "        print(\"No queries were generated\")\n",
        "\n",
        "    return simple_result and (len(queries) > 0)\n",
        "\n",
        "# Run the test\n",
        "success = await test_run()\n",
        "print(f\"\\nOverall test {'succeeded' if success else 'failed'}\")\n",
        "\n",
        "# If tests pass, ask to run full document processing\n",
        "# if success and input(\"\\nRun full document processing? (y/n): \").lower() == 'y':\n",
        "#     chunks = await debug_main()"
      ],
      "metadata": {
        "id": "VX2evw9tyTLL",
        "outputId": "c33711ee-47ba-438d-e333-cf3d55d47f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Individual Components ===\n",
            "\n",
            "--- Test 1: Simple Completion ---\n",
            "Testing simple completion...\n",
            "Response received: { \t\"text\" \t: \t\"hello world\" \t}\n",
            "Successfully parsed response: {'text': 'hello world'}\n",
            "Simple completion test result: True\n",
            "\n",
            "--- Test 2: Entity Extraction ---\n",
            "Testing with text:\n",
            "Wire transfers exceeding $50,000 require dual authorization from designated officers.\n",
            "The primary authorizer must be a department manager or above.\n",
            "Entity extraction raw response: \n",
            "\n",
            " \t{ \n",
            " \t\"entities\" \t: \t[ \n",
            " \t               \t\"Wire transfers\" \t               \t, \t               \t\"$50,000\" \t               \t, \t               \t\"dual authorization\" \t               \t, \t               \t\"officers\" \t               \t, \t               \t\"department manager\" \t               \t] \n",
            " \t}\n",
            "Successfully extracted 5 entities\n",
            "Extracted entities: ['Wire transfers', '$50,000', 'dual authorization', 'officers', 'department manager']\n",
            "\n",
            "--- Test 3: Query Generation ---\n",
            "Generating queries for test chunk...\n",
            "Query generation raw response:    \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "   \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "Error generating queries: Expecting value: line 1607 column 4 (char 4550)\n",
            "Full exception: JSONDecodeError('Expecting value: line 1607 column 4 (char 4550)')\n",
            "No queries were generated\n",
            "\n",
            "Overall test failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug generator initialization\n",
        "print(\"\\n=== Testing Generator Configuration ===\")\n",
        "print(f\"API Key present: {api_key is not None}\")\n",
        "print(f\"Base URL: {base_url}\")\n",
        "print(f\"Model: {generator.model}\")\n",
        "\n",
        "# Run the main process with proper async handling\n",
        "async def debug_main():\n",
        "    try:\n",
        "        print(\"\\n=== Starting Debug Process ===\")\n",
        "\n",
        "        # Test 1: Simple completion\n",
        "        print(\"\\n--- Test 1: Simple Completion ---\")\n",
        "        completion_result = await generator.test_simple_completion()\n",
        "        print(f\"Simple completion test result: {completion_result}\")\n",
        "        if not completion_result:\n",
        "            return\n",
        "\n",
        "        # Test 2: Entity extraction\n",
        "        print(\"\\n--- Test 2: Entity Extraction ---\")\n",
        "        entity_result = await generator.test_entity_extraction()\n",
        "        print(f\"Entity extraction test result: {entity_result}\")\n",
        "        if entity_result is None:\n",
        "            return\n",
        "\n",
        "        # Main processing\n",
        "        print(\"\\n--- Main Document Processing ---\")\n",
        "        chunks = await generator.process_document(policy_document)\n",
        "\n",
        "        # Results summary\n",
        "        print(\"\\n=== Results Summary ===\")\n",
        "        print(f\"Total chunks processed: {len(chunks)}\")\n",
        "        total_queries = sum(len(chunk.queries) for chunk in chunks)\n",
        "        print(f\"Total queries generated: {total_queries}\")\n",
        "\n",
        "        # Save results\n",
        "        generator.save_ground_truth(chunks, 'ground_truth.json')\n",
        "        print(\"\\nProcessing complete - results saved to ground_truth.json\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! Error in debug_main: {str(e)}\")\n",
        "        print(f\"Full exception: {repr(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute with proper async handling\n",
        "chunks = await debug_main()"
      ],
      "metadata": {
        "id": "2azhApIUxpuB",
        "outputId": "29ebd25a-a728-465f-c50e-7f69682e2603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Generator Configuration ===\n",
            "API Key present: True\n",
            "Base URL: https://wesslen--vllm-openai-compatible-serve.modal.run/v1\n",
            "Model: /models/NousResearch/Meta-Llama-3.1-8B-Instruct\n",
            "\n",
            "=== Starting Debug Process ===\n",
            "\n",
            "--- Test 1: Simple Completion ---\n",
            "Testing simple completion...\n",
            "Response received: { \n",
            " \t\"text\" \t: \t\"hello world\" \n",
            "}\n",
            "Successfully parsed response: {'text': 'hello world'}\n",
            "Simple completion test result: True\n",
            "\n",
            "--- Test 2: Entity Extraction ---\n",
            "Testing entity extraction...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NfyVNAV9wS52"
      }
    }
  ]
}